<section>
    <section class="no_bg">
        <h2>Introduction to Kubernetes</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Any App, Anywhere?</h2>

        <p>Containers are portable. What does this imply about the best ways to manage a containerized data center?</p>

        <aside class='notes'>
            <ul>
                <li>Lead class to consider the advantage of letting containers get scheduled anywhere, rather than controlling exactly where they get scheduled. Then, explore what the consequences of what spreading containers across multiple hosts are (need for management, control and data planes; need for some special-case control over scheduling decisions).</li>
                <li>Hint questions if the class is stuck:</li>
                <li>In general, does it matter where a given container gets scheduled? Why or why not? (conclude that in most cases, it doesn't matter, but there may be special cases such as needing specific hardware, or being sensitive to resource availability).</li>
                <li>If we're scheduling containers on any arbitrary host in our datacenter, we're going to need some sort of cluster manager in order to administrate all these hosts and containers. How will we provide service discovery? (Consider things like peer-to-peer service discovery (no choke points but could generate a lot of traffic), hub and spoke distribution (potentially less traffic but single points of failure), lookup from a kv store (just-in-time lookup could minimize traffic but might block traffic depending on implementation). Others?)</li>
                <li>Once we've solved the service discovery problem, how are we going to get packets from one host to another? Recall that so far, we've only seen linux bridges moving packets around at layer 2.</li>
                <li>Enabling this full-datacenter scheduling and solving the corresponding and communication challenges are the fundamental responsibilities of any production-ready container orchestrator.</li>
            </ul>
        </aside>
    </section>
    
    <!--
    <section class="no_bg">
        <h2>Discussion: Container Communication</h2>

        <p>Suppose you have two services you want to guarantee can reach each other on the same host, to eliminate network latency. How would you do this in Swarm?</p>

        <aside class='notes'>
            <ul>
                <li>Concrete example: an API tier that needs to hit a database tier - don't want to wait for network latency between the two.</li>
                <li>Entertain suggestions from the class for a while until concluding that there's no way to do this with swarm services. Kubernetes solves the same fundamental problems as any orchestrator, but gives an alternative networking and scheduling model that supports different use cases than Swarm.</li>
            </ul>
        </aside>

    </section>
    -->

    <section class="no_bg">
        <h2>Learning Objectives</h2>
        
        <p>By the end of this module, learners will be able to</p>

        <ul>
            <li>Understand the components and roles of Kubernetes masters and nodes</li>
            <li>Identify and explain the core Kubernetes objects (Pod, ReplicaSet, Deployment, Service, Volumes)</li>
            <li>Provision configuration via configMaps and secrets</li>
            <li>Explore the Kubernetes networking model</li>
        </ul>
    </section> 

    <section class="no_bg">
        <h2>Orchestrator Goals</h2>

        <p><span class='keyword'>Top-line goal:</span> operate a datacenter like a <span class='keyword'>pool of compute resources</span> (not individual machines). This requires</p>

        <ul>
            <li>Add / remove compute resources securely and easily</li>
            <li>Schedule containers across the cluster transparently</li>
            <li>Streamline container-to-container communication (service discovery, load balancing and routing)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>The core goal of any orchestrator is to let you treat a bunch of computers like a pool of compute resources ready to support your containerized workloads; the fact that containers are so portable and work the same way anywhere means we generally don't have to care too much what specific node they get scheduled on. Instead, we'd like to delegate that decision to our orchestrator, and think about our cluster as a whole rather than as a lot of individual machines.</li>
                <li>In order to actually do that, there are some minimal requirements any orchestrator will need to satisfy:</li>
                <li>Not only do we have to be able to add new nodes to our cluster, this has to be done in a way that is secure at join time, and secure in communication long term.</li>
                <li>Our orchestrator will need to be able to schedule workload anywhere in the cluster; this is perhaps the easiest part, thanks to the portability of containers. Beyond this, our orchestrator should also be responsible for automatically maintaining that workload as much as possible.</li>
                <li>Finally, with our applications potentially divided among many containers and hosts, we'll need our orchestrator to help us communicate between these containers, by facilitating service discovery, load balancing and routing appropriate to the networking needs of each of our applications.</li>
                <li>There are of course many more things an orchestrator could do for us, but these are the core concerns that any feasible orchestrator must do.</li>
            </ul>
        </aside>
    </section>
    <section class="no_bg">
        <h2>Pets Versus Livestock</h2>

        <div class='row'>
            <div class='col-4'>
                <ul>
                    <li>Kubernetes reschedules exited containers automatically</li>
                    <li style='margin-right:1em'>When a container becomes unhealthy, <span class='keyword'>kill it and get a new one</span>.</li>
                </ul>
            </div>
            <div class='col-8'>
                <img src='images/petsvslivestock.png' style="background:none !important;">
                 <figcaption style="font-size:50%; line-height:0; text-align:center; margin-top: 1em;">Dog photo <a href="https://www.flickr.com/photos/jeffreyww/4975374886/in/photolist-8zE6BC-c9tWCb-c9tXfd-5a7MyF-c3KLuL-5dDirY-axQTra-c9sgSG-amLcYQ-c3KFn7-c9tXo1-c9tWc9-dHZUz-rgAhXo-rW2iU1-KSM6e-pSCtLH-qF7ff3-5Sqz9E-fmSgbs-7KxKak-8TZKk8-6gVmH9-ehfejV-ehtrx4-bD41t2-aiMDUn-8U3iS9-DacH2L-63pHfi-8TZfEH-93nazZ-dmdgPv-85pGjL-8p9Un7-8p9Ui3-5a7EUt-8qP6BK-345kg4-8BRoqc-65p6Gq-YkNMy4-8J4rLP-o2EFGs-H43vF2-8U3jm3-8U48Fb-Si8zjf-8U48pY-2dBGh2">jeffreyw</a>; Livestock photo <a href="https://www.flickr.com/photos/pauljill/28350728097/in/photolist-KcfTJR-a38m8q-tNhS3j-27TRuXP-Ssmqi7-9uD1Xw-PyTy8A-4D77Jm-8pSu5Q-W3FcwF-ZypKkz-Ue3XvX-VP9jfs-28pSeWb-UP2ryi-LQt3rp-UT391d-MEqR17-Y7sweh-8X7vrU-Y7FJfm-QBydX8-FgTo81-XZr5cN-SPXUwJ-rhVK-bmDYEB-dPFa64-287ET1L-UZyj2p-TcrWzo-ZzPMvn-8JAYv1-48S7W5-8C3qEJ-J6FjAR-urgqiC-f4E4kU-DTLaj9-24prtP3-6P3yWW-28ynA9Q-UPjBk8-UyDxAW-XwKZRW-24HpVUY-ehvBKf-HbXbmw-BSCQ2s-8Uz7pW"> Paul Asman, Jill Lenoble</a>; images <a href='https://creativecommons.org/licenses/by/2.0/'>CC-BY 2.0</a></figcaption>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>In the exercises and demos so far, we've seen how Swarm will reschedule containers after they exit. This is a crucial feature for how we think about managing orchestrated container workloads.</li>
                <li>We often describe this mindset as livestock management, versus pet care. With a pet, we concern ourselves with every injury and illness, and attempt to keep our pet healthy and happy as long as possible; this is the wrong way to think about troubleshooting containers. As any rancher knows, what matters is not the health of an individual livestock animal, but the health of the herd. If one animal gets sick, the right course of action is to kill it and get a new one. The same is true with containers; if a container scheduled by swarm misbehaves, the first course of action to take is to kill it and let Swarm reschedule it.</li>
                <li>If pathologies persist after rescheduling fresh containers, then a more serious debugging effort makes sense - but not when a single container gets rescheduled or has to be restarted.</li>
                <li>Bear in mind this is usually a big change from how we thought about managing VMs - VMs were more like pets we want to keep alive. Bringing the same management and troubleshooting mindset to the containerization world is a common mistake that causes new container users a lot of unnecessary pain.</li>
            </ul>
        </aside>
    </section> 

    <section class="no_bg">
        <h2>Kubernetes Master</h2>
        <div class="row">
            <div class="col-7">
                <p>Important Components</p>
                <ul>
                    <li><span class="keyword">API Server:</span> Frontend into Kubernetes control plane</li>
                    <li><span class="keyword">Cluster Store:</span> Config and state of cluster</li>
                    <li><span class="keyword">Controller Manager:</span> Assert desired state</li>
                    <li><span class="keyword">Scheduler:</span> Assigns workload to nodes</li>
                </ul>
            </div>
            <div class="col-5">
                <img src="images/master.png" title="master" style="margin-left: 20px;">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>The API Server (apiserver) is the frontend into the Kubernetes control plane. It exposes a RESTful API that preferentially consumes JSON. We POST manifest files to it, these get validated, and the work they define gets deployed to the cluster.</li>
                <li>The config and state of the cluster gets persistently stored in the cluster store, which is the only stateful component of the cluster and is vital to its operation - no cluster store, no cluster!<br> The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store. As it is the single source of truth for the cluster, you should take care to protect it and provide adequate ways to recover it if things go wrong.</li>
                <li>The controller manager (kube-controller-manager) implements things like the node controller, endpoints controller, namespace controller etc. They tend to sit in loops and watch for changes – the aim of the game is to make sure the current state of the cluster matches the desired state</li>
                <li>At a high level, the scheduler (kube-scheduler) watches for new workloads and assigns them to nodes. Behind the scenes, it does a lot of related tasks such as evaluating affinity and anti-affinity, constraints, and resource management.</li>
                <li>Note that directly analogous components appear in a Swarm manager; in Swarm's case they are integrated directly into the Docker engine, but the same roles and responsibilities appear in both. This is the first example of what we meant by Kube being more modularly designed.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Node</h2>
        <div class="row">
            <div class="col-7">
                <ul>
                    <li><span class="keyword">Kubelet:</span> Kubernetes Agent</li>
                    <li><span class="keyword">Container Engine:</span> Host Containers</li>
                    <li><span class="keyword">Network Proxy:</span> Networking &amp; Load Balancing</li>
                </ul>
            </div>
            <div class="col-5">
                <img src="images/node.png" title="master" style="margin-left: 20px;">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Kubelet: This is the main Kubernetes agent that runs on all cluster nodes. When kubelet is installed on a Linux host then it registers the host with the cluster as a node. It then watches the API server for new work assignments. Any time it sees one, it carries out the task and maintains a reporting channel back to the master.</li>
                <li>If the kubelet can’t run a particular work task, it reports back to the master and lets the control plane decide what actions to take. For example, if a Pod fails on a node, the kubelet is not responsible for restarting it or finding another node to run it on. It simply reports back to the master. The master then decides what to do.</li>
                <li>Container Engine: The Kubelet needs to work with a container runtime to do all the container management stuff – things like pulling images and starting and stopping containers. More often than not, the container runtime that Kubernetes uses is Docker. In the case of Docker, Kubernetes talks natively to the Docker Remote API.</li>
                <li>More recently, Kubernetes has released the Container Runtime Interface (CRI). This is an abstraction layer for external (3rd-party) container runtimes to plug in to. Basically, the CRI masks the internal machinery of Kubernetes and exposes a clean documented container runtime interface. The CRI is now the default method for container runtimes to plug-in to Kubernetes. The containerd CRI project is a community-based open-source project porting the CNCF containerd runtime to the CRI interface.</li>
                <li>Kube Proxy: The last piece of the puzzle is the kube-proxy. This is like the network brains of the node. For one thing, it makes sure that every Pod gets its own unique IP address. It also does lightweight load-balancing on the node.</li>
            </ul>
        </aside>
    </section>
        
    <section class="no_bg">
        <h2>Architecture</h2>
        <img src="images/architecture.png" title="Architecture">
        <aside class="notes">
            <ul>
                <li>In this schema we have one master and 3 nodes. On the master we find the 4 services that we discussed earlier: API service, scheduler, controller manager and cluster storage</li>
                <li>On each of the nodes we have kubelet, Kubernetes proxy and the container host such as containerd. The container host runs containers C1, ..., Cx</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Orchestration Objects</h2>

        <img src="images/deployment.png" title="Deployment">

        <aside class='notes'>
            <ul>
                <li>While Docker Swarm provides a complete orchestration stack in its service objects, Kubernetes defines several different entities that must be combined to achieve the same result. This is the other way in which kubernetes is more modular than swarm, in that its orchestration abstractions are broken down into multiple objects.</li>
                <li>Pod: In Kubernetes a Pod is the atomic unit of scheduling. We cannot run containers directly on a Kubernetes cluster.</li>
                <li>At the highest-level, a Pod is a ring-fenced environment to run containers. The Pod itself doesn’t actually run anything, it's just a sandbox to run containers in. Keeping it high level, we ring-fence an area of the host OS, build a network stack, create a bunch of kernel namespaces, and run one or more containers in it.</li>
                <li>If one runs multiple containers in a Pod, they all share the same environment - things like the IPC namespace, shared memory, volumes, network stack etc. As an example, this means that all containers in the same Pod will share the same IP address (the Pod’s IP).</li>
                <li>ReplicaSet: A ReplicaSet is a higher-level Kubernetes object that wraps around a Pod and adds features. As the names suggests, they take a Pod template and deploy a desired number of replicas of it. They also instantiate a background reconciliation loop that checks to make sure the right number of replicas are always running – desired state vs actual state. ReplicaSets can be deployed directly. But more often than not, they are deployed indirectly via even higher-level objects such as Deployments.</li>
                <li>Deployment: Deployments provide broader desired state management to ReplicaSets. After updating the declarative desired state of replicasets in the JSON or YAML that describes a Deployment, those updates will be rolled out in a controlled fashion (ie a rolling update).</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pods</h2>

        <img src="images/podstructure.png" title="Pods" style='max-width:50%'>
        <p><span class='keyword'>Logical host</span> supporting interacting processes</p>

        <aside class='notes'>
            <ul>
                <li>Perhaps the most fundamental conceptual difference between Docker and Kubernetes is the notion of a pod. While Docker's fundamental scheduling unit is a container - a single process with the full stack of isolation technologies applied to it - kubernetes's atomic unit is the pod, which is meant to model a logical host supporting multiple interacting processes.</li>
                <li>At the kernel namespace level, pods reduce some of the isolation between containers by sharing their network, IPC, and UTS namespaces. This allows all the containers within a pod to interact with each other as if they were all sitting on the same host.</li>
                <li>As such, all the containers within a pod share a single IP address, port range, routing table, hostname, and unix sockets. This is the first example of what we meant by kubernetes prioritizing its communication model over its security model.</li>
                <li>Resource limitations by way of control groups are imposed at the pod level; security features like capabilities management, SecComp and security modules are bundled together as securityContext objects (container context takes precedence over pod context).</li>
                <li>All pods contain something called a pause container, which is responsible for keeping the shared namespaces of the pod open even if all other processes in the pod exit or restart.</li>
            </ul>
        </aside>

    </section>    

    <section class="no_bg">
        <h2>Pods</h2>
        <div class="row">
            <div class="col-7">
                <img src="images/pod-addr.png" title="Pods">
            </div>
            <div class="col-5">
                <ul>
                    <li>Use <code>localhost</code> for intra-pod communication</li>
                    <li>All containers in a pod share same IP</li>
                </ul>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Since all containers in a pod share the same network namespace and they all pertain to the same (pod) IP address, containers can just communicate with each other via "localhost".</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pod Networking</h2>
        <img src="images/docker-vs-k8s.png" title="Pod Networking">
        <aside class="notes">
            <ul>
                <li>The Docker network:</li>
                <li>On top we have a physical network interface called "eth0". Attached to that is a Linux bridge "docker0", and attached to that is a virtual network interface "veth0". It is important to note that "docker0" and "veth0" are both on the same network, 172.5.0.0/24 in this example. On this network "docker0" is assigned the IP address 172.5.0.1 and it is the default gateway for "veth0", which is assigned the IP address 172.5.0.2.</li>
                <li>The second container gets a new virtual network interface "veth1", connected to the same "docker0" bridge. In our case it is assigned the IP address 172.5.0.3</li>

                <li>Kubernetes Network:</li>
                <li>Docker can start a container and rather than creating a new virtual network interface for it, specify that it shares an existing interface.</li>
                <li>The command looks like this: "docker container run --name bar ... --net container:foo ..."</li>
                <li>The above command shares the network namespace of container "foo" with the new container "bar".</li>
                <li>Now the second container "bar" sees "veth0" rather than getting its own "veth1" as in the previous example.</li>
                <li>In this way multiple containers live in the same network space and can communicate with each other via `localhost`. This is similar to what we know from the situation when we run multiple processes directly on the host.</li>
                <li>Kubernetes implements this pattern by creating a special container "pause" for each pod whose only purpose is to provide a network interface for the other containers.</li>
                <li>The “pause” container is the heart of the pod, providing the virtual network interface that all the other containers will use to communicate with each other and the outside world.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pod Lifecycle</h2>

        <img src="images/pod-lifecycle.png" title="Pod Lifecycle">

        <aside class="notes">
            <ul>
                <li>A pod has a lifecycle. Since a it is also an atomic unit of deployment the following is valid:</li>
                <li>Pending: containers are still spinning up (ie being scheduled by kube, and images being downloaded if necessary).</li>
                <li>Running: pod is bound to a node, and at least one container is still running or restarting.</li>
                <li>Succeeded: all containers exited with exit code 0</li>
                <li>Failed: all containers exited, at least one with a non-zero exit code.</li>
                <li>Note the monotonous progression: unlike containers, pods don't stop and restart. They live until they die.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="images/icon_task.png" class="moby_icon" alt="icon">Instructor Demo: Kubernetes Basics</h2>
        
        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='kubernetes-demo.md'>Kubernetes Basics</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section class="no_bg">
        <h2>ReplicaSet</h2>

        <div class="row">
            <div class="col-8">
                <img src="images/replicaset.png" title="ReplicaSet">
            </div>
            <div class="col-4">
                <ul>
                    <li>Scaling</li>
                    <li>Keep-alive</li>
                </ul>
            </div>
        </div>

        <aside class="notes">
            <ul>
                <li>ReplicaSets bring the concepts of desired number of replicas and self-healing to a collection of Pods. Just like in Swarm when a service task dies, a dead pod will be rescheduled by a ReplicaSet.</li>
                <li>We define ReplicaSets with either a YAML or a JSON manifest file and feed it to the API server. This gets handed over to the ReplicaSet controller which makes sure the right number of the right Pod get instantiated. Fundamental to this is the all-powerful reconciliation loop that is constantly watching the cluster and making sure that current state and desired state match.</li>
                <li>Even if we only need a single instance of a Pod, we should probably deploy it via a higher-level object like a ReplicaSet or Deployment. This will give the Pod self-healing capabilities and the ability to scale if we decide we need more in the future.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Deployment</h2>
        <div class="row">
            <div class="col-6">
                <img src="images/deployment.png" title="Deployment">
            </div>
            <div class="col-6">
                <ul>
                    <li>Build on top of <span class="keyword">ReplicaSets</span></li>
                    <li>Add configurable Updates and Rollback</li>
                    <li>Older Versions of ReplicaSets stick around for easy Rollback</li>
                </ul>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Deployments build on top of Pods and ReplicaSets by adding mature and configurable updates and rollbacks.</li>
                <li>Like everything else, they’re objects in the Kubernetes API and we should be looking to work with them declaratively.</li>
                <li>When we perform updates with the kubectl apply command, older versions of ReplicaSets get wound down, but they stick around making it easy for us to perform rollbacks.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Configuration</h2>

        <p>Applications typically need environment-specific config:</p>
        <ul>
            <li>Environment variables</li>
            <li>Configuration files</li>
            <li>Non-sensitive info (ports, usernames, endpoints)</li>
            <li>Sensitive info (passwords, access tokens, keys)</li>
        </ul>

        <p>Config should be <span class='keyword'>decoupled</span> from pod definition and <span class='keyword'>portable</span> across the cluster.</p>

        <aside class='notes'>
            <ul>
                <li>In addition to scheduling workload as containers and pods, we will typically also need to provide configuration info for our applications.</li>
                <li>We'd like to be able to define the config in a way that keeps it well separated from the definition of our pods, so that the same pods and deployments can be migrated across environments without changing their definition at all; we should be able to swap out modular configuration objects to completely capture changes in config.</li>
                <li>Also, these modular configuration objects need to be managed by our orchestrator so they can be as portable as our pods themselves, able to be provisioned anywhere in our cluster.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>ConfigMaps</h2>

        <ul>
            <li>Collections of key/value pairs, or text files</li>
            <li>Provisioned to containers via env vars or volume mounts</li>
            <li>Appropriate for low/no security config</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>For any non-sensitive configuration information, a configMap satisfies our desires for decoupled, portable configuration.</li>
                <li>ConfigMaps can be defined as lists of key/value pairs, which can be used to populate environment variables in containers, or as lists of files which can be used to populate volume mounts.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Secrets</h2>

        <ul>
            <li>Defined and provisioned similarly to configMaps (env vars or volume mounts)</li>
            <li>Intended for secure info:
                <ul>
                    <li>Provisioned in a tmpfs, never written to disk</li>
                </ul>
            </li>
            <li><span class='keyword'>Warning</span>: secrets are recoverable with <code>kubectl get secrets</code> from masters, and potentially with <code>docker container inspect</code> from host workers</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>For config that requires higher security guarantees, Kubernetes provides a construct parallel to configMaps called secrets.</li>
                <li>Secrets are used similarly to configMaps, in that they are defined separately from pods, and can populate environment variables and mounted files in a running container. Secrets are not, however, written to disk, and are deleted from the tmpfs they are stored in on worker nodes once the container consuming them is deleted.</li>
                <li>Furthermore, UCP sets up default Kube secret encryption on install via the aescbc provider discussed at <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"></a>this site.</li>
                <li>Be aware that anyone with `kubectl get secrets` permissions on your cluster can recover the value of a secret from a master. Secrets can also be exposed via environment variables visible through `docker container inspect` on nodes hosting pods that consume a secret as an environment variable. Therefore, while secrets improve the security posture of sensitive information, proper RBAC and limited access to cluster nodes is still crucial for security.</li>
            </ul>
        </aside>
    </section>   

    <section class="no_bg">
        <h2>Storage Volumes</h2>

        <ul>
            <li>Volumes: same lifecycle as pod (compare to persistent Docker volumes)</li>
            <li>PersistentVolumes: 'immortal' volume (similar to Docker)</li>
            <li>Storage backends exposed by <span class='keyword'>container storage interface</span> drivers</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Similar to Docker containers, we often want storage for our applications that outlives the containers that isolate our processes; for this we have Kube volumes.</li>
                <li>One critical difference between Kube and Docker's volume solutions is that Kube volumes are created alongside pods, and do not outlive those pods; once a pod is deleted, the volumes it created are also destroyed. Compare this to a Docker volume, which is created independently from a container and by default outlives any container that mounts it.</li>
                <li>If we want a more 'Docker-like' persistent storage solution, PersistentVolumes provide volumes that are provisioned separately from pods, and persist past the lifecycle of any pods that mount them.</li>
                <li>Kubernetes abstracts away arbitrary third-party storage solutions using the Container Storage Interface, a generic standard for orchestrator storage drivers.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Kubernetes Orchestration</h2>
        
        <p>Work through</p>

        <ul>
            <li class='exercise' script='kube-install.md'>Installing Kubernetes</li>
            <li class='exercise' script='kube-orchestration.md'>Kubernetes Orchestration</li>
            <li class='exercise' script='provisioning-kube-config.md'>Provisioning Kube Config</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>

    </section>

    <section class="no_bg">
        <h2>Kubernetes Network Model</h2>
        <img src="images/kube-networking.png" alt="Kube Networking">
        <p>Requirements</p>
        <ul>
            <li>Pod &lt;--&gt; Pod without NAT</li>
            <li>Node &lt;--&gt; Pod without NAT</li>
            <li>Pod's peers find it at the same IP it finds itself</li>
            <li>Creates a <span class='keyword'>flat network</span>, like VMs</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>Kubernetes does not have a strict opinion on how networking between pods has to be implemented. It only requests the following 3 characteristics of a valid networking implementation:</li>
                <li>(1) All pods can communicate with all other pods without NAT</li>
                <li>(2) All nodes running pods can communicate with all pods (and vice-versa) without NAT</li>
                <li>(3) IP that a pod sees itself as is the same IP that other pods see it as</li>
                <li>In our sample we have two nodes on a subnet 172.10.0.0/16 and all pods are on subnet 10.1.0.0/16 while node1 has subnet 10.1.1.0/24 and node2 has 10.1.2.0/24 reserved for its respective pods.</li>
                <li>According to the requirements (1) pod A needs to be able to reach pod B, pod C and pod D without NAT. (2) Furthermore node 1 and 2 need to be able to reach all pods A to D. (3) Finally pod A sees its own IP as 10.1.1.2 and all other pods in the network see pod A with the same IP 10.1.1.2 and can reach it accordingly.</li>
                <li>To elaborate a bit on the 3rd point: processes running inside any container of pod A see their IP as 10.1.1.2.</li>
                <li>Kube's networking model is most distinct from the Docker native CNM in its priorities; Kubernetes wanted a networking model that most closely resembled a flat network of VMs. By demanding all containers sit on a flat network and can all reach each other, Kubernetes forgoes Docker's secure by default firewalling but makes it very simple to port an application previously distributed across networked VMs into a collection of pods that are nearly identical from a networking perspective. This is the other sense in which kubernetes prioritized its communication model over its security model.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Service</h2>
        <div class="row">
            <div class="col-6">
                <p>Problem:</p>
                <ul>
                    <li>Pods are mortal</li>
                    <li>Pods are never resurrected</li>
                    <li>Pod IP Addr cannot be relied on</li>
                </ul>
                <p>Solution:</p>
                <ul>
                    <li><span class"keyword">Service</span> defines: 
                        <ul>
                            <li>Logical set of Pods</li>
                            <li>Policy how to access them</li>
                        </ul> </li>
                </ul>
            </div>
            <div class="col-6">
                <img src="images/service.png" title="Service">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Just like service tasks managed by Swarm, pods managed by Kubernetes come and go, either from failures, scheduling decisions, updates, or scaling.</li>
                <li>We don't want our application logic to have to watch out for these potentially fast-changing operational details; we'd rather abstract away a pool of identical pods behind a stable endpoint, which is what a Kubernetes service provides.</li>
                <li>A kube service is a fully-fledged object in the Kubernetes API just like Pods, ReplicaSets, and Deployments. They provide stable IP addresses, and support TCP and UDP (TCP by default). They also perform simple randomized load-balancing across Pods, though more advanced load balancing algorithms may be supported in the future. This adds up to a situation where Pods can come and go, and the Service automatically updates and continues to provide that stable networking endpoint.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>ClusterIP Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='images/clusterip.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster internal origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm VIP</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>The most basic Kubernetes service is the clusterIP service. Traffic sent to the service IP and port will get randomly load balanced across all matching pods, on the service's targetPort port.</li>
                <li>Note this is for cluster internal communications only; the service IP will only be reachable from other pods running on the same cluster.</li>
                <li>Also note, the random routing implies this is appropriate only for stateless destination pods.</li>
                <li>Finally, recall that UCP automatically deploys a kube DNS service - so the IP of your clusterIP services, like all services, can be resolved by a DNS lookup of the service name, again from within the originating pod.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>NodePort Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='images/nodeport.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster external origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm L4 mesh</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>Building off of the clusterIP service is the nodePort service, intended for routing traffic from outside the cluster to your pods.</li>
                <li>nodePort services automatically create clusterIP services that work exactly as described above; the nodePort service itself listens on a randomly assigned port on every node in the cluster, and forwards traffic inbound there to the clusterIP service, which in turn hands it off to the matching pods as above.</li>
                <li>Note that nodePort services listen on EVERY host in the cluster, regardless of what pods are running where; therefore, inbound traffic doesn't need to be targeted at a specific node. Your external load balancer could in principle just fan traffic across the whole cluster on the port selected by your nodePort service, and the Kube services will handle the rest of the routing.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Label Selectors</h2>

        <img src="images/labelselector.png" title="label selector" style='width:70%'>

        <aside class='notes'>
            <ul>
                <li>Since a Kubernetes service is a completely independent object, how do we connect it to the pods it's supposed to route traffic to?</li>
                <li>We address this problem with kube label selectors. Pods bear a metadata -> labels key, beneath which can contain any arbitrary label and value; meanwhile, services bear a selector -> matchLabels key beneath which resides the same label and value combination.</li>
                <li>You already saw this in the exercises when you created replicaSets and deployments; the higher level objects were matched to pods in the exact same way. In the examples you declared the pods simultaneously with the orchestration objects, but any pod, even ones declared separately, with a matching label will get picked up and managed by the replicaSet, deployment or service.</li>
                <li>In order to restrict what objects match what other objects in this way, Kubernetes also offers namespacing functionality for objects; in this workshop we'll just put everything in the default namespace, but in practice you might consider running multiple versions of an app (say dev and staging) in different namespaces, so the dev services don't point at staging pods, or vice versa.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Network Policies</h2>
        <div class="row">
            <div class="col-7">
                <ul>
                    <li>Network policies <span class="keyword">control traffic</span></li>
                    <li>Traffic allowed by default</li>
                    <li>Traffic denied if network policy exist but no rule allows it</li>
                    <li>Independent <span class="keyword">ingress</span> &amp; <span class="keyword">egress</span> rules</li>
                </ul>
            </div>
            <div class="col-5">
                <div class="pre large">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ...
  namespace: ...
  ...
spec:
  <span class="red-bg">podSelector:</span> ...
  <span class="red-bg">ingress:</span>
  - ...
  - ...
  <span class="red-bg">egress:</span>
  - ...
  - ...</div>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Network policies control traffic from and to pods</li>
                <li>Kubernetes defines policies but ignores them silently IF no supporting network plugin is installed.</li>
                <li>Calico is such a network plugin that Docker includes "as batteries included" in UCP 3.0. Calico supports network policies and is commercially backed by Tigera.</li>
                <li>Policies work based on labels (on pods). In a policy we have label selectors that define which pods are affected by the policy</li>
                <li>An empty label selector means that ALL pods are included: matchLabels: {}</li>
                <li>In addition to which pods it applies to a policy also defines which direction of traffic is affected: ingress (inbound traffic; who can access the pod) or egress (outbound traffic; where can the pod connect to)</li>
                <li>On the slide we see the template of a network policy with the podSelector and the ingress and egress rules.</li>
                <li>Traffic is allowed unless there is a network policy selecting the pod</li>
                <li>Traffic is denied if there are policies selecting the pod but none of them have rules allowing it</li>
                <li>We can only write rules to ** allow ** traffic</li>
                <li>Traffic is allowed if there is at least one policy allowing it</li>
                <li>Policies can also restrict port numbers and protocols, e.g. 3456/tcp</li>
                <li>Policy rules are additive (logical OR)</li>
                <li>Multiple pod selectors are also additive (logical OR)</li>
                <li>Network policies are scoped to their namespace (the one they're deployed to)</li>
                <li>Network policies add minimal latency (less than 1 ms overhead): http://blog.kubernetes.io/2016/09/high-performance-network-policies-kubernetes.html</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Sample Network Policies</h2>
        <img src="images/network-policies.png" alt="Sample Network Policy">
        <aside class='notes'>
            <ul>
                <li>Here we see a sample network policy that allows some traffic while denying other. Only components from the backend tier can communicate with the DB tier but not e.g. frontend components</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Networking Planes</h2>

        <ul>
            <li>
                Management:
                <ul>
                    <li>Master to Master: etcd Raft</li>
                    <li>Master to Node: apiserver (TCP 6443) &lt;-&gt; kubelet (TCP 10250)</li>
                </ul>
            </li>
            <li>
                Data &amp; Control:
                <ul>
                    <li>BYO networking</li>
                    <li>See Cluster DNS, <a href='http://bit.ly/2DMmdyt'>http://bit.ly/2DMmdyt</a></li>
                </ul>
            </li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Like a Swarm, Kubernetes needs a management, control, and data plane to orchestrate fully distributed applications.</li>
                <li>Kube's management plane consists of two distinct parts: etcd replicas across masters maintain a Raft consensus, and masters' apiserver communicates with node's kubelet.</li>
                <li>Unlike Swarm, Kubernetes control and data planes are largely bring-your-own, with the details governed by networking solutions like calico, flannel or weave.</li>
                <li>Also see the Cluster DNS addon for Kubernetes; this will provision a DNS server that registers DNS names for Kubernetes services and pods based on their names and namespaces.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Calico</h2>

        <img src="images/calico-routing.png" alt="bgp / ip in ip">

        <aside class='notes'>
            <ul>
                <li>Every kubernetes network plugin implements L3 routing differently; here we illustrate Calico, since that's the one that ships with UCP by default.</li>
                <li>Calico assigns a /26 subnet to each node in the cluster (a node can get additional /26 subnets as they fill up - but a particular /26 will belong to exactly one host.)</li>
                <li>All pods on a host receive IPs in this /26 subnet allocated by the calico IPAM.</li>
                <li>Each pod's network namespace is connected to the host network namespace via a veth, just like in Docker.</li>
                <li>Calico amends the host's routing table to direct traffic bound for a local pod to the corresponding veth endpoint, named 'cali*'.</li>
                <li>Calico also runs a BGP server (BIRD) in an all-to-all mesh, updating the routing tables of all other nodes in the cluster with information about which subnets have been assigned to which nodes. Then, if any traffic is destined for a subnet not corresponding to the local node, the destination node's IP is inferred from the /26 subnet, and the packet is sent via IP-in-IP.</li>
                <li>In this sense, Calico is using BGP as its networking control plane (compare to gossip in Swarm), and IP in IP as its data plane (compare to VXLAN)</li>
                <li>In the case of very large clusters, the all-to-all BGP mesh can become unperformant. It's also possible to configure calico route reflectors to serve as the 'hub' in a hub-and-spoke control plane, where all nodes route outbound traffic to the route reflectors rather than directly to its destination, and the route reflectors then pass the traffic to its destination. This way, the control plane scales linearly in cluster size rather than like n*n, though at the cost of having a central hub for all intra-cluster communications (needs high bandwidth, potential point of failure).</li>
            </ul>
        </aside>

    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Kubernetes Networking</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='kube-networking.md'>Kubernetes Networking</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>

    </section> 
    
    <section class="no_bg">
        <h2>Kubernetes Takeaways</h2>
        <ul>
            <li>Kubernetes provides more flexibility in its orchestration objects at the cost of more config</li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>The key strategic things to understand when thinking about kube are its modularity, and its communication model.</li>
                <li>Compared to Swarm, Kube is much more modular, providing a great deal of flexibility in scheduling decisions. However, as is generally true for most software, added flexibility often results in more complicated config.</li>
                <li>Also comparing to swarm, kube offers a potentially powerful communication model via the concept of pods and its globally reachable network model. If your app requires very chatty containers that need interprocess communication or colocation, pods are an excellent design decision offered by kubernetes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Docker &amp; Kubernetes: <a href="https://www.docker.com/kubernetes">https://www.docker.com/kubernetes</a></li>
            <li>Official Kubernetes Docs: <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a></li>
            <li>Tutorials: <a href="http://bit.ly/2yLGn61">http://bit.ly/2yLGn61</a></li>
            <li>Interactive Tutorials: <a href="https://bit.ly/2rdwIVZ">https://bit.ly/2rdwIVZ</a></li>
            <li>Understanding Kubernetes Networking: <a href="http://bit.ly/2kdI1qQ">http://bit.ly/2kdI1qQ</a></li>
            <li>Kubernetes the Hard Way: <a href="http://bit.ly/29Dq4wC">http://bit.ly/29Dq4wC</a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>Additional resources about Kubernetes</li>
            </ul>
        </aside>
    </section>
</section>
