<!doctype html>
<html lang="en">

    <head>
        <!-- Global site tag (gtag.js) - Google Analytics -->
        <script async src="https://www.googletagmanager.com/gtag/js?id=UA-155731101-1"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'UA-155731101-1');
        </script>
        
        <meta charset="utf-8">
    
        <title>Gepardec</title>

        <meta name="description" content="Gepardec">
        <meta name="author" content="Clemens Kaserer">

        <meta name="apple-mobile-web-app-capable" content="yes">
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">

        <!-- css -->
        <link rel="stylesheet" href="css/reveal.css">
        <link rel="stylesheet" href="src/css/gepardec.css" id="theme">
        <link rel="stylesheet" href="src/css/gepardec-custom.css">
        <link rel="stylesheet" href="src/css/gepardec-code.css">

        <!-- Printing and PDF exports -->
        <script>
            var link = document.createElement( 'link' );
            link.rel = 'stylesheet';
            link.type = 'text/css';
            link.href = window.location.search.match( /print-pdf/gi ) ? 'src/css/print/pdf.css' : 'src/css/print/paper.css';
            document.getElementsByTagName( 'head' )[0].appendChild( link );
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->
    </head>

    <body>
        <!--<img src="images/flecken.png" style="width: auto; height: 300px; position: fixed; bottom: 10px; right: -50px; z-index: -9999; opacity: 0.4;" alt="gepardec flecken">-->
        <img src="images/GEPARDEC_logo_RGB_hellerhg.svg" style="width: auto; height: 50px; position: fixed; top: 30px; right: 30px; z-index: -9999;" alt="gepardec logo">
        <p style="font-size:12px !important; z-index:10; color:rgb(0, 0, 0); position: fixed; bottom: 30px; left: calc(50% - 8em);"><i>#WECKDENGEPARDENINDIR</i></p><!--Â© 2019 // Gepardec IT Services GmbH-->
		<div class="reveal">
            <div class="slides">
                <section>
    <section data-background-image="src/modules/Training-for-Containerization/01-welcome/images/gepard_white.png" data-background-size="auto 100%" data-background-position="left">
        <h2>Containerization<br>Training</h2>
    </section>

    <section class="no_bg">
        <h2>How We Teach</h2>
        <ul>
            <li>gepardec believes in <b>learning by doing</b></li>
            <li>The training is <b>lab driven</b></li>
            <li>Work together!</li>
            <li>Ask questions at any time</li>
        </ul>       

        <aside class="notes">
            <ul>
                <li>This workshop is primarily exercise based</li>
                <li>Lecture will be limited and focus on the high-level concepts, best practices, and ideas we want to tell you about</li>
                <li>Most of our time will be spent on demo exercises, designed to illustrate the usage, syntax, and details of all the tools we explore, and map onto the learning objectives for each chapter</li>
            </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Session Logistics</h2>
        <ul>
            <li>2 days duration</li>
            <li>Mostly exercises</li>
            <li>Regular breaks</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Assumed Knowledge and Requirements</h2>
        <ul>
            <li>Familiarity with Bash or Powershell</li>
            <li>Bash Cheat Sheet: <a href='http://bit.ly/2mTQr8l'>http://bit.ly/2mTQr8l</a></li>
            <li>Powershell Cheat Sheet: <a href='https://bit.ly/2EPHxze'>https://bit.ly/2EPHxze</a></li>
        </ul>

        <aside class='notes'>
            <p>Basic familiarity with:</p>
            <ul>
                <li>Filesystem navigation and manipulation: ls, cd, mv, cp, rm</li>
                <li>Tooling: ssh, top, chmod, curl, wget</li>
                <li>Package management with yum</li>
                <li>And powershell equivalents</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Your Lab Environment</h2>
        <ul>
            <li>You have been given an instances for use in exercises</li>
            <li>Ask instructor for credentials if you don't have them already</li>
        </ul>

        <aside class='notes'>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Course Learning Objectives</h2>

        <p>By the end of this course, learners will be able to</p>
        <ul>
            <li>Assess the advantages of a containerized software development &amp; deployment</li>
            <li>Use Docker engine features necessary for running containerized applications</li>
            <li>Utilize Kubernetes orchestrators to deploy, maintain, and scale a distributed application</li>
        </ul>
    </section>  
</section>

<section>
    <section class="no_bg">
        <h2>Introducing Docker</h2>
        <aside class="notes"><h3>An intro module to get students on message with what docker is, and the priorities and concerns of distributed application dev and ops.</h3>
        </aside>
    </section>

    <section class="no_bg">
        <h2>What We Want</h2>

        <p>Ideal software should</p>

        <ul>
            <li>be modular and flexible (devs)</li>
            <li>be easy to migrate (devops)</li>
            <li>be easy to scale, monitor and lifecycle (ops)</li>
            <li>mitigate vulnerabilities (security)</li>
            <li>run cheap (business)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Every stakeholder in the software supply chain has a set of priorities they'd like software to satisfy.</li>
                <li>Developers want flexibility and choice in the components they build application out of; problems like vendor lock-in, technical debt, and tight coupling between components slows the development cycle down and can prevent developers from building the software they want.</li>
                <li>Devops engineers in charge of building and maintaining CI/CD pipelines are primarily responsible for getting applications running across many environments; they'd like to be able to move software easily, and rely that tests passing in one environment won't break in another.</li>
                <li>Operations teams responsible for scaling, maintaining and monitoring software would like to be able to deploy applications easily across a datacenter, and know what to expect from those applications to make monitoring and maintenance simple.</li>
                <li>Security teams want assurances that software not only minimizes attack surfaces, but has built-in failsafes to mitigate compromises when they occur.</li>
                <li>Finally, business interests want to be able to do all of the above as cheaply as possible, by making most efficient use of the compute resources purchased to run it.</li>
                <li>These are many and varied priorities - but containerization is such a success because it speaks to all of them.</li>
            </ul>
        </aside>
    </section>


    <section class="no_bg" style="top: 79.5px; display: block;">
        <h2>Without Containerization</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/pre-container.png' width="80%"></img>

        <aside class='notes'>
            <ul>
                <li>Without containerization, we can imagine a host that looks as such. The key feature of this diagram is that all the dependencies, configurations, and system resources for applications on this host are shared.</li>
                <li>From the developer's standpoint, an uncontainerized environment requires strict discipline to avoid tight coupling between components; it's easy to develop your way into a state where changes to one component break another, and all components are affected by the environmental requirements of the others, complicating and hindering testing and development.</li>
                <li>For a devops engineer, the shared host environment of these components also adds friction. Every environment in the CI pipeline has to reflect this stack of dependencies and configurations, which can be difficult to maintain and difficult to reproduce.</li>
                <li>For operations personnel, the more tightly coupled different components become, the more difficult they can be to scale and monitor; tight couplings can make it complicated to understand what's needed to relax a performance bottleneck or trace root causes of application failures.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>With Containerization</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/post-container.png' width="80%"></img>

        <aside class='notes'>
            <ul>
                <li>The fundamental insight of the containerization movement was similar to that of the real-life logistics chains from which containerization got its name: many problems are solved by strictly encapsulating software as we develop, migrate, and deploy it.</li>
                <li>The key feature of a software container is that it is self-contained, all the way down to the OS filesystem; not only our executables, but all their configs and all their dependencies, and even the OS filesystem they run in, are captured in a container that can be moved as a complete unit.</li>
                <li>Modern Linux and Windows kernels can even represent the host system differently to different containers, presenting them different network devices, process trees, filesystems and more; all you need installed on the host is the Docker engine.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Rapid Development</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/component-upgrade.png'></img>
        <p>Containers can be removed and replaced with a minimum of impact on their neighbors, increasing developer choice and speed.</p>

        <aside class='notes'>
            <ul>
                <li>From the developer's point of view, aggressive encapsulation equals speed and flexibility; since containers provide assurances that one component doesn't affect another, developers can upgrade one component while remaining confident they won't break others.</li>
                <li>Remember that containers carry the full dependency stack of the application they're designed to run, all the way down to the OS filesystem; this means that developers can use different stacks for different components, all on the same host with no chance of dependency conflicts.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Smooth Migration</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/migration.png'></img>
        <p>Containers carry their environment and dependencies with them, simplifying and minimizing requirements on the hosts that run them.</p>

        <aside class='notes'>
            <ul>
                <li>From the devops point of view, aggressive encapsulation equals smooth migrations; since containers carry everything they need with them, they impose relatively few requirements on the host that runs them. Hosts need only provide a compatible kernel and architecture, and a container engine; no application specific dependencies or environment configurations need be maintained across environments.</li>
                <li>In this way, not only will a container that runs in dev likely run in prod, it will run the same way; containerized software passing tests in one environment has a better chance of passing tests in all environments, since environment matters comparatively little once we containerize our applications.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Simple Scale &amp; Maintenance</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/scalability.png'></img>
        <p>Weak coupling between containers minimizes side effects when scaling and simplifies monitoring.</p>

        <aside class='notes'>
            <ul>
                <li>From the point of view of operations personnel, the encapsulation provided by containers translates into simple methodology for scaling and maintaining applications.</li>
                <li>Scale is achieved by containers in part by the minimal requirements they put on their environment; since containers are designed to run the same way regardless of their hosting environment, multiple instances can be spun up in parallel, or distributed across all the hosts in a datacenter.</li>
                <li>Furthermore, a well-designed container should run only a very limited set of tasks; by avoiding tight coupling and defining relatively simple behaviors for each container, it becomes simpler to define what 'healthy' means for a container, compared to a complete host with many interacting processes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Secure by Default</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/isolation.png'></img>
        <p>Containers have private system resources, so a compromise in one does not affect the rest.</p>

        <aside class='notes'>
            <ul>
                <li>From the point of view of security, aggressive encapsulation automatically enhances our security posture.</li>
                <li>On an uncontainerized host, if an attacker successfully compromises one component with elevated privileges, they can leverage those privileges across the entire host, using any vulnerable component as a point of ingress.</li>
                <li>On the other hand, containerized software mitigates its own vulnerabilities; even if an attacker compromises one container via a vulnerable component, privileges gained there do not grant the same level of access to other containers or to the host; root in a container does not equal root on the host.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Application Density</h2>

        <img src='src/modules/Training-for-Containerization/02-docker-story/images/density.png'></img>
        <p>Containers save datacenter costs by running many more application instances than virtual machines can on the same physical hosts.</p>

        <aside class="notes">
            <ul>
                <li>In terms of operational costs, containers can radically reduce the amount of metal needed to run a given workload compared to a virtual machine, because they are so resource efficient; containerized processes use the host kernel and don't require fixed allocations of CPU and memory, so can be run with the absolute minimal footprint.</li>
                <li>It's not unusual for the same datacenter to be able to run 10x as many containers encapsulating a given process, compared to VMs encapsulating the same software, resulting in substantial cost savings to users.</li>
            </ul>
        </aside>
    </section>

    <!--
    <section class="no_bg">
        <h2>The Containerization Stack</h2>

        <div style='text-align:center !important'>
            <img style='width:60%' src='src/modules/Training-for-Containerization/02-docker-story/images/container-stack.png'></img>
        </div>

        <aside class='notes'>
            <ul>
                <li>Any containerization stack needs three elements:</li>
                <li>A container runtime to actually make containers</li>
                <li>An orchestration layer to network containers together, potentially across remote hosts</li>
                <li>An enterprise tooling layer that provides for the management and security needs of large enterprises.</li>
                <li>Docker Community Edition provides the container runtime, and an orchestrator (based on SwarmKit), free for anyone to use. Kubernetes is also available as an alternative, powerful orchestration layer.</li>
                <li>Docker Enterprise Edition goes beyond the free product to provide tools like a secure API, role based access control, and tooling for creating container-based CI/CD pipelines.</li>
                <li>In this course, we're going to study the first two layers of the containerization stack; join us for our more advanced courses that dive into our enterprise tooling.</li>
            </ul>
        </aside>
    </section>
    -->
</section>
<section>
    <section class="no_bg">
        <h2>Containerization Basics</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Running Containers</h2>

        <p>What assurances would you need to run a process on an arbitrary host? Consider</p>
        <ul>
            <li>Hostile environments</li>
            <li>Required resources</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about the need for process isolation (via kernel namespaces etc), filesystem provisioning (via Docker images), and resource consumption limitations (via cgroups)</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Do you trust the other processes running on this host? Should they trust you? (leads to need for process isolation)</li>
                <li>Will your process' dependencies be available on this host? Are you sure? (leads to need for docker images)</li>
                <li>How much resources can you reasonably consume on this host? (leads to need for cgroup limitations)</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Describe what a container is in terms of processes and isolation tools</li>
            <li>Use the key commands for interacting with Docker containers</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Containers are Processes</h2>

        <p><span class='keyword'>Containers</span> are processes sandboxed by</p>
        <ul>
            <li>Kernel namespaces</li>
            <li>Control Groups</li>
            <li>Root privilege management &amp; syscall restrictions (Linux)</li>
            <li>VM isolation (Windows)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Fundamentally, containers are just processes living on the host, isolated primarily by kernel namespaces, and control groups for resource isolation.</li>
                <li>Linux containers can also be subject to restrictions on what root privileges and system calls they are allowed to make, by application of a linux security module.</li>
                <li>On the Windows side, containers enhance their isolation by running in ultra-light-weight VMs.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Architecture in Linux</h2>
        <div class="col-6">
            <img src="src/modules/Training-for-Containerization/03-container-basics/images/architecture-linux.png" title="Linux Architecture" class="transparent">
        </div>
    <!--
            <div class="col-6">
                <p>Architecture in Windows</p>
                <img src="src/modules/Training-for-Containerization/03-container-basics/images/architecture-windows.png" title="Windows Architecture" class="transparent">
            </div>
        </div>
    -->    <aside class="notes">
            <ul>
                <li>On a high level the architectures of Linux and Windows are not really that much different. Both rely on fundamental kernel features for creating containers like kernel namespaces and control groups.</li>
                <li>On the Linux side, a container runtime such as containerd uses these kernel features to create and run containers.</li>
                <li>On the Windows side, Microsoft has introduced the Compute Service to abstract the low level details of the capabilities like control groups, namespaces and layer capabilities and not make them (yet) public so that the Windows team can quickly iterate on those features without breaking any external contracts. In this regard Compute Service is the public interface the Docker platform communicates with.</li>
                <li>As said, this is a public API and thus not only Docker is using it but there exists multiple bindings, e.g. for C# and for Go (which is used by Docker).</li>
                <li>C#: https://github.com/Microsoft/dotnet-computervirtualization</li>
                <li>Go: https://github.com/Microsoft/hcsshim</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Linux Kernel Namespaces</h2>

        <ul>
            <li><b>DEFAULT</b>
                <ul>
                    <li>Process IDs</li>
                    <li>Network stacks</li>
                    <li>Inter-process communications</li>
                    <li>Mount points</li>
                    <li>Hostnames</li>
                </ul>
            </li><br>
            <li><b>OPTIONAL</b>
                <ul>
                    <li>User IDs</li>
                </ul>
            </li>
        </ul>

        <aside class="notes">
            <ul>
                <li>The baseline tool for creating containers is the kernel namespace. Kernel namespaces create distinct representations of things like PID trees, user spectra, network stacks and mount points; processes live in exactly one namespace, and are only able to interact with the broader system via the representation encapsulated therein.</li>
                <li>By analogy: an un-namespaced system is like when airplanes used to have just one TV in the cabin everyone would look up at to watch the in-flight movie. Everyone shared the device, and everyone saw the same thing.</li>
                <li>Introducing namespaces is like putting seatback TVs in front of every passenger. Now everyone has their own private devices and sees their own thing, which is hidden from their neighbors.</li>
                <li>In the same way, a namespaced process has all their own resources - their own iptables rules and eth0 device, their own mount points, their own PID tree - and processes in other namespaces aren't allowed to touch or even see these resources.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux PID Kernel Namespace</h2>

        <img src='src/modules/Training-for-Containerization/03-container-basics/images/pid-tree.png' style='width:70%'></img>

        <aside class="notes">
            <ul>
                <li>For example, PID namespaces make the first process in the namespace appear as the root of a process tree to all other processes in that namespace, which will be its children.</li>
                <li>Meanwhile, processes in the parent namespace see these processes with PID numbers like any other process in the parent PID namespace.</li>
                <li>In this way, processes in the child namespace aren't able to find information about processes in the parent namespace, but the child namespace remains transparent from the perspective of the parent namespace.</li>
                <li>Stopping the PID 1 of a child namespace and stopping the container are the exact same thing.</li>
                <li>Isolating host system resources, rather than creating a whole new virtual machine, is where the high performace of containers comes from. Think of it like building a little wall around a patch of sand in a sandbox; the area marked off can itself be thought of as a new sandbox, but no new sand has been acquired.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Optional Linux Isolation Features</h2>

        <ul>
            <li>Control groups: limit memory &amp; CPU</li>
            <li>Root privilege management: whitelist root powers</li>
            <li>System call management: whitelist available system calls</li>
            <li>Linux Security Modules: mandatory filesystem access control</li>
        </ul>

        <aside class='notes'>
            <li>In addition to the default kernel namespaces, a number of other linux technologies can be imposed on your containers to restrict their privileges.</li>
            <li>All production deployments should take advantage of control groups, to limit how much memory and CPU a container can consume.</li>
            <li>Root privileges and system calls are governed by whitelist, which can be configured per-container</li>
            <li>Linux security modules allow the creation of access control rules for filesystem paths and objects within the container.</li>
        </aside>

    </section>    

    <!--
    <section class="no_bg">
        <h2>Windows: Host Kernel Containers</h2>
        <img src="src/modules/Training-for-Containerization/03-container-basics/images/windows-containers.png" style='width:70%' title="Windows Containers" class="transparent">
        <aside class="notes">
            <ul>
                <li>Windows Containers can share the same Windows Kernel of the underlying Windows OS (either Windows Server 2016 or Windows 10).</li>
                <li>A fundamental difference between Linux and Windows containers is the infrastructure they require to make low-level calls to the kernel. In Linux, a container will have a single application process, which can simply issue system calls. But in Windows, a small collection of system processes are required alongside the application process to provide the same functionality; just as these system processes run in uncontainerized user mode, they must also be available inside a windows container.</li>
                <li>The implementation details of Windows containers are fundamentally different than those of linux containers, but they mimic the functionality and isolation provided by kernel namespaces and control groups.</li>
                <li>Microsoft has done and continues to do a lot of tremendous work to minimize this overhead, particularly with Nanoserver.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Windows: Hyper-V Containers</h2>
        <img src="src/modules/Training-for-Containerization/03-container-basics/images/hyper-v-containers.png" style='width:70%' title="Hyper-V Containers" class="transparent">
        <aside class="notes">
            <ul>
                <li>Windows containers alone don't have a set of isolation tools analogous to root privilege management or linux security modules.</li>
                <li>This has led to Hyper-V Containers. Hyper-v containers are Windows Containers that are wrapped inside a very special and slim VM. This means that from a developers or DevOps perspective nothing changes. These containers are built the exact same way as normal Windows containers and it is merely a switch in the `docker container run` command which determines in which mode the container should run.</li>
                <li>Hyper-V Containers provide all the security and isolation to a single container that we are used to when using VMs to isolate our applications. Yet due to special optimizations Hyper-V containers are much leaner and faster than traditional VMs and thus can be packed much more densely on a server.</li>
                <li>Hyper-V containers have one workload and deliver added isolation for multi-tenant or hostile environments</li>
                <li>The isolation provided by hyper-v containers provides a Windows alternative to the added isolation provided by linux security modules and root privilege management on the Linux side.</li>
            </ul>
        </aside>
    </section>
    -->
    
    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/03-container-basics/images/icon_task.png" class="moby_icon" alt="icon">Instructor Demo: Process Isolation</h2>
        
        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='process-isolation-demo.md'>Process Isolation</li>
        </ul>

        <p>In the Exercises book.</p>
        
    </section> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/03-container-basics/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Container Basics</h2>
        <p>Work through</p> 
        <ul>
            <li class='exercise' script='running-and-inspecting-containers.md'>Running and Inspecting a Container</li>
            <li class='exercise' script='interactive-containers.md'>Interactive Containers</li>
            <li class='exercise' script='detached-containers-and-logging.md'>Detached Containers and Logging</li>
            <li class='exercise' script='starting-stopping-inspecting-and-deleting-containers.md'>Starting, Stopping, Inspecting and Deleting Containers</li>
        </ul>

        <p>In the Exercises book.</p>

        <h2 class="timer"></h2>
        
    </section>

    <section class="no_bg">
        <h2>Container Lifecycle</h2>

        <img src='src/modules/Training-for-Containerization/03-container-basics/images/container-lifecycle.png' style='width:80%'></img>

        <aside class="notes">
            <ul>
                <li>The rectangles display the state of the container and the arrow labels show the Docker command used to change the container state.</li>
                <li>The container lifecycle always begins in the CREATED state. A container in this state has a private filesystem set up on disk (more on this in the next chapter) and metadata defined regarding what process it is to encapsulate and how, but it not yet running. When the process in question begins running, the container transitions to the UP state.</li>
                <li>If a containerized process exits, the container transitions to the EXITED state. It can normally be restarted with a start command.</li>
                <li>Finally, Docker containers can enter a PAUSED state of suspension imposed by control group freezing. This suspension technique (unlike using SIGSTOP and SIGCONT) can't be caught by the process, ensuring that pausing a container doesn't disrupt the process it containerizes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Container Logs</h2>

        <ul>
            <li>STDOUT and STDERR for a containerized process</li>
            <li><code>docker container logs &lt;container name&gt;</code></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>STDOUT and STDERR of whatever process is PID 1 in a container are logged by Docker, and available via `docker container logs`</li>
            </ul>
            
        </aside>
    </section>

    <section class="no_bg">
        <h2>Container Basics Takeaways</h2>
        <ul>
            <li>Single process constrained by kernel namespaces, control groups and other technologies</li>
            <li>Private &amp; ephemeral filesystem and data</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>The key conceptual take-aways for container basics are the first two points; the container's main process, which it labels PID 1, will stop the container when it itself stops; and writing to the container's file system writes only to that container, not the underlying image; soon we'll learn more about these underlying images, and how to manipulate them.</li>
                <li>Everything else we learned in this unit is basic creation, deletion and investigation syntax.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>List of container commands: <a href="http://dockr.ly/2iLBV2I">http://dockr.ly/2iLBV2I</a></li>
            <li>Getting started with containers: <a href="http://dockr.ly/2gmxKWB">http://dockr.ly/2gmxKWB</a></li>
            <li>Start containers automatically: <a href="http://dockr.ly/2xB8sMl">http://dockr.ly/2xB8sMl</a></li>
            <li>Limit a container's resources: <a href="http://dockr.ly/2wqN5Nn">http://dockr.ly/2wqN5Nn</a></li>
            <li>Keep containers alive during daemon downtime: <a href="http://dockr.ly/2emLwb5">http://dockr.ly/2emLwb5</a></li>
            <li>Isolate containers with a user namespace: <a href="http://dockr.ly/2gmyKdf">http://dockr.ly/2gmyKdf</a></li>
            <li>Intro to Windows Containers: <a href="https://dockr.ly/2CTYhYb">https://dockr.ly/2CTYhYb</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about containers</li>
            </ul>
            
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Creating Images</h2>

        <aside class="notes">
            <ul>
                <li>In our discussion of containers, we focused on how processes are isolated.</li>
                <li>But if a process is to be truly portable, it also needs its filesystem and dependencies to come along with it.</li>
                <li>In this module, we'll explore images in-depth, including a focus on creating and modifying images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Provisioning Filesystems</h2>

        <p>What are some potential difficulties with provisioning entire filesystems for containers? How can we avoid these problems?</p>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about some of the non-obvious nuances around building and sharing images</li>
                <li>Obvious answer: disk and bandwidth usage</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Are all container filesystems necessarily unique? If two containers share a filesystem, how will they remain independent? (leads to layer sharing and read-only images)</li>
                <li>What's a simple way to minimize the risk of vulnerable components making it onto your production servers via a Docker image? (answer: don't install any component you don't absolutely need, indicates the logic of minimal images)</li>
                <li>If containers and their filesystems are meant to move across environments like dev, testing, staging and prod without needing to be modified, how can they reflect the important differences between those environments? (leads to motivating multi-stage builds)</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Create images via several methods</li>
            <li>Describe the filesystem structure underlying an image</li>
            <li>Understand the performance implications of different image design decisions</li>
            <li>Correctly tag and namespace images for distribution on a registry</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>What are Images?</h2>

        <div class='row'>
            <div class='col-6'>
                <ul>
                    <li>A <span class="keyword">filesystem</span> for container process</li>
                    <li>Made of a stack of <span class="keyword">immutable</span> layers</li>
                    <li>Start with a <span class="keyword">base image</span></li>
                    <li>New layer for each change</li>
                </ul>
            </div>
            <div class='col-6' style='text-align:right'>
                <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-1.png'></img>
            </div>            
        </div>

        <aside class="notes">
            <ul>
                <li>Images are composed in layers; each layer consists of a bunch of files that capture how this layer adapts the one beneath it.</li>
                <li>Note: On Windows there are also Windows registy entries that are captured as part of the layer.</li>
                <li>These stacks of layers always start with a base image, which typically captures only the base operating system for this image.</li>
                <li>each subsequent image layer captures sequential changes to the image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Sharing Layers</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-2.png'></img>

        <aside class="notes">
            <p>benefits of layering: (see if students can guess):</p>
            <ul>
                <li>Sharing layers == smaller on disk and in memory</li>
                <li>Sharing layers == faster downloads (de-duped by default)</li>
                <li>Allows caching when constructing images</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>The Writable Container Layer</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/lfs-3.png'></img>

        <aside class="notes">
            <ul>
                <li>Starting a container essentially adds a single writable layer to the image stack; since Docker is just adding this one thin layer, container startup is very fast and resource efficient.</li>
                <li>Any manipulations of the filesystem a container does is written only to this R/W layer; all image layers are always read-only.</li>
                <li>When a container edits a file from the base image, then and only then is that file copied to the R/W layer; this is what is meant by Docker's 'copy on write' filesystem; this also implies that the copy of a file that is visible in a running container is whichever copy of that file sits highest in the stack of filesystem layers.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Images: Copy on Write</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/copy-on-write.png' style='width:80%'></img>

        <aside class='notes'>
            <ul>
                <li>The final product is composed per this diagram, via what we call a 'copy on write' composition strategy</li>
                <li>Each time a layer is added, only files that are changed are copied up to the next layer; each of these layers actually exists as a directory on your host machine.</li>
            </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Linux Containers: Union FS</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/linux-ufs.png' style='width:70%'></img>

        <aside class='notes'>
            <ul>
                <li>When creating a container on Linux, a R/W container layer is created, and all these filesystem layers are composed via a union filesystem mount. This assembles the image layers into a unified filesystem similar to superimposing a stack of overhead transparencies on top of each other; files on higher layers obscure earlier versions of themselves on lower layers.</li>
                <li>When a container modifies a file from the image, it performs the same copy on write action as above, into the R/W container layer.</li>
            </ul>
        </aside>

    </section>

    <!--
    <section class="no_bg">
        <h2>Windows Containers: Linked FS</h2>

        <img src='src/modules/Training-for-Containerization/04-creating-images/images/windows-fs.png' style='width:70%'></img>

        <aside class='notes'>
            <ul>
                <li>On the Windows side, making union file systems backwards compatible with NTFS wasn't feasible.</li>
                <li>Instead, Windows uses reparse points to effectively link the appropriate image files into a container layer; if a file is unmodified in the layer, a stub is created for it containing the reparse point. Actually, each image layer also contains reparse points to earlier reparse points, in a chain down to the original file.</li>
                <li>When a Windows container changes a file in an image, it's copied to the virtual hard drive where the changes can be made.</li>
            </ul>
        </aside>
    </section>
    -->
    
    <section class="no_bg">
        <h2>Creating Images</h2>
        <p>Three methods:</p>
        <ul>
            <li><span class="keyword">Commit</span> the R/W container layer as a new R/O image layer.</li>
            <li>Define new layers to add to a starting image in a <span class="keyword">Dockerfile</span>.</li>
            <li><span class="keyword">Import</span> a tarball into Docker as a standalone base layer.</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Committing Container Changes</h2>
        <ul>
            <li><code>docker container commit</code><br>saves container layer as new R/O image layer</li>
            <li>Pro: build images interactively</li>
            <li>Con: hard to reproduce or audit; <span class='keyword'>avoid this</span> in practice.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>One way of building up images is to save the container layer as a new image layer</li>
                <li>This is fine for experiments, but it's really something best avoided in the development of production grade code, since it isn't easily auditable, reproducible or automated.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfiles</h2>
        <ul>
            <li>Content manifest</li>
            <li>Provides image layer documentation</li>
            <li>Enables automation (CI/CD)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Interactive image creation is good for tinkering, but its main drawback is that it doesn't produce an artifact describing the steps to create the image in a machine-readable way.</li>
                <li>Therefore, there's no way to build images this way as part of a CI/CD chain, and it can be hard to audit what exactly is in the image.</li>
                <li>A Dockerfile is essentially a recipe to build an image, layer by layer. This can be ingested in build processes and CI/CD pipelines, and preserves a record of all the steps taken to create an image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfiles</h2>
        <ul>
            <li><code>FROM</code> command defines base image.</li>
            <li>Each subsequent command adds a layer or metadata</li>
            <li><code>docker image build ...</code> builds image from Dockerfile</li>
        </ul>
        <br><br>
        <div class="row">
            <div class="col-6">
                <div class='pre'># Comments begin with the pound sign
<span class="red-bg">FROM</span> ubuntu:16.04
<span class="red-bg">RUN</span> apt-get update &amp;&amp; apt-get install -y wget
<span class="red-bg">ADD</span> /data /myapp/data
...</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre"># Comments begin with the pound sign
<span class="red-bg">FROM</span> microsoft/nanoserver:latest
<span class="red-bg">RUN</span> Install-Module -Name Nuget -Force
<span class="red-bg">ADD</span> c:\\myapp\data c:\\app\data
...</div>
            </div>
        -->
        </div>

        <aside class='notes'>
            <ul>
                <li>Note that dockerfiles for linux and windows are syntactically identical; they use different images for their bases and run different processes at each step, but the way we specify our image recipe doesn't change at all.</li>
            </ul>
        </aside>
    </section>

    <section  data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercises: Creating Images</h2>
        <p>Work through</p> 
        <ul>
            <li class='exercise' script='interactive-image-creation.md'>Interactive Image Creation</li>
            <li class='exercise' script='creating-images-with-dockerfiles-part-1.md'>Creating Images with Dockerfiles (1/2)</li>
        </ul>
        <p>in the Exercises book.<p>
        <h2 class="timer"></h2>
    </section> 

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Creating Images</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='creating-images-demo.md'>Creating Images</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 
    
    <section class="no_bg">
        <h2>Build Cache</h2>
        <div class='row'> 
            <div class='col-4'>
                <img src='src/modules/Training-for-Containerization/04-creating-images/images/building-images.png' style='width:80%'></img>
            </div>
            <div class='col-8'>
                <ul>
                    <li>After completion, the resulting image layer is labeled with a hash of the content of all current image layers in the stack.</li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>Layers are fetched from the cache via the hash label affixed to that layer the first time it was created.</li>
                <li>Q: Why is a hash for a layer computed based on the entire image? Why not just that layer?</li>
                <li>A: A layer can't be reused unless all layers under it are the same; put another way, the effect of whatever command generated the layer might be different depending on substrate layers.</li>
                <li>The upshot being that the builder will stop using the cache at the first change in the Dockerfile.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>CMD and ENTRYPOINT</h2>
        <ul>
            <li>Recall all containers run a process as their PID 1</li>
            <li><code>CMD</code> and <code>ENTRYPOINT</code> allow us to specify default processes.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Another pair of helpful commands in Dockerfiles are CMD and ENTRYPOINT</li>
                <li>These are used for specifying default processes and options to run in containers created from this image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>CMD and ENTRYPOINT</h2>
        <ul>
            <li><code>CMD</code> alone: default command and list of parameters.</li>
            <li><code>CMD</code> + <code>ENTRYPOINT</code>: <code>ENTRYPOINT</code> provides command, <code>CMD</code> provides default parameters.</li>
            <li><code>CMD</code> overridden by command arguments to <code>docker container run</code>
            <li><code>ENTRYPOINT</code> overridden via <br><code>--entrypoint</code> flag to <code>docker container run</code>.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Oftentimes images are designed to do exactly one thing; CMD and ENTRYPOINT allow you to bake that intention right into the image, by pre-specifying that command.</li>
                <li>The difference between the two is essentially in how you want to override these defaults</li>
                <li>Using them together makes your container feel a lot like an executable; arguments (defaulted by CMD) will be overridden by command line args, but the executable defined by ENTRYPOINT will not.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Shell vs. exec format</h2>
        <div class="row">
            <div class="col-6">
                <div class='pre'># Shell form
CMD sudo -u ${USER} java ...

# Exec form
CMD ["sudo", "-u", "jdoe", "java", ...]</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class='pre'># shell form
CMD c:\\Apache24\\bin\\httpd.exe -w

# exec form
CMD ["c:\\Apache24\\bin\\httpd.exe", "-w"]</div>
                <p>Note the "\\" in the expressions</p>
            </div>
            -->
        </div>
        <aside class='notes'>
            <ul>
                <li>CMD, ENTRYPOINT and RUN commands can use either exec or shell syntax</li>
                <li>If we have a command like this on Windows `powershell New-Item c:\test` then if it is in declared in shell form what is executed is in reality `cmd /S /C powershell New-Item c:\test` whilst in exec form the command is executed as is without the use of the shell (cmd in this case). The analogous is true for Linux containers.</li>
                <li>exec is generally preferred for ENTRYPOINT, since it preserves the ability to override options.</li>
                <li>
                    subtle differences:
                    <ul>
                        <li><i>Shell form</i> allows for the parsing of variables like <code>CMD sudo -u ${USER} java ... </code></li>
                        <li><i>Exec form</i> can run in a container with no shell; shell form always runs via <code>/bin/sh -c</code></li>
                        <li><i>Shell form</i> for <code>ENTRYPOINT</code> prevents options from being overridden by <code>CMD</code> or <code>docker container run</code>.</li> 
                    </ul>
                </li>
                <li>Note that exec form is formal JSON - double quotes mandatory.</li>
                <li>When using the shell form, the specified binary is executed with an invocation of the shell using /bin/sh -c, which means the process running as PID 1 is the /bin/sh executable.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Dockerfiles (2/2)</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='creating-images-with-dockerfiles-part-2.md'>Creating Images with Dockerfiles (2/2)</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section>

    <section class="no_bg">
        <h2>COPY and ADD commands</h2>

        <p><code>COPY</code> copies files from build context to image</p>
        <pre class='large'>COPY &lt;src&gt; &lt;dest&gt;</pre>

        <div>
            <p><code>ADD</code> can also <span class='keyword'>untar</span>* and <span class='keyword'>fetch URLs</span>.</p>
            <div style="font-style: italic; font-size: 0.5em !important; color:darkgrey !important;">* Linux containers only!</div>
        </div>    
        <p>In both cases</p>
        <ul>
            <li>create checksum for files added</li>
            <li>log checksum in build cache</li>
            <li>cache invalidated if checksum changed</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>COPY and ADD add files from the local filesystem to the image</li>
                <li>Build process uses a checksum against the files to be added to bust the cache if those files have changed</li>
                <li>Note that ADD can also copy files from a URL and for Linux containers only(!) untar files upon copying them into the image.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Dockerfile Command Roundup</h2>

        <ul>
            <li><span class='keyword'>FROM</span>: base image to start from (usually OS)</li>
            <li><span class='keyword'>RUN</span>: run a command in the environment defined so far</li>
            <li><span class='keyword'>CMD</span> and <span class='keyword'>ENTRYPOINT</span>: define default behavior</li>
            <li><span class='keyword'>COPY</span> and <span class='keyword'>ADD</span>: copy files into container</li>
        </ul>

        <p>Many more Dockerfile commands are available; see the docs at <a href="https://docs.docker.com/engine/reference/builder/"><span ></span>https://docs.docker.com/engine/reference/builder/</span></a></p>

        <aside class='notes'>
            <ul>
                <li>We've seen the greatest hits of Dockerfile commands, but there are tons more; see the docs.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Advanced Dockerfile Construction</h2>

        <p>How can we build images that are</p>

        <ul>
            <li>Lightweight</li>
            <li>Secure</li>
            <li>Minimal build times</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Now that we've seen the basics of image construction with Dockerfiles, we'd like to investigate best practices around image construction</li>
                <li>Our priorities for image creation are size, security, and build times.</li>
                <li>Size and security can be addressed by similar techniques; making sure we only install things we absolutely need in our image not only keeps the image size down, but avoids exposing ourselves to potential vulnerabilities in superfluous components.</li>
                <li>Also during the course of development, we'd like build times to be as fast as possible, either by leveraging the cache we've already seen, or by parallelizing parts of the build process.</li>
                <li>For the next part of this chapter, we'll look at some advanced techniques for achieving all of these.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>The Scratch Image</h2>
        <ul>
            <li>An "empty" image</li>
            <li>Can't be pulled</li>
            <li>Doesn't create a layer</li>
            <li>Used for building images not based on any pre-existing image</li>
            <li>Linux only</li>
        </ul>

        <div class='pre'>FROM <span class="red-bg">scratch</span>

ADD centos-7-docker.tar.xz /

LABEL org.label-schema.schema-version="1.0" \
org.label-schema.name="CentOS Base Image" \
org.label-schema.vendor="CentOS" \
org.label-schema.license="GPLv2" \
org.label-schema.build-date="20181205"

CMD ["/bin/bash"]</div>

        <aside class='notes'>
            <ul>
                <li>The scratch image is an empty image that exists in Docker Hub, but has no tags and can't be pulled.</li>
                <li>When used in a Dockerfile, the line `FROM scratch` doesn't add any layer to the image. The next command in the Dockerfile will be the first filesystem layer.</li>
                <li>The scratch image is used typically to build base images with as few components as possible installed in them, to give the smallest possible attack surface to our images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Multi-Stage Builds</h2>

        <div class="row">
            <div class="col-6">
                <div style="font-size: 0.5em;">Hello World, in C:</div>
                <div class="pre">FROM alpine:3.5
RUN apk update &amp;&amp; \
    apk add --update alpine-sdk
RUN mkdir /app
WORKDIR /app
ADD hello.c /app
RUN mkdir bin
RUN gcc -Wall hello.c -o bin/hello 
CMD /app/bin/hello</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div style="font-size: 0.5em;">Hello World, in Go:</div>
                <div class="pre">FROM golang:nanoserver</span>
COPY . /code
WORKDIR /code
RUN go build hello.go
CMD ["\\code\\hello.exe"]</div>
            </div>
-->
        </div>
        <br>
        <div style="font-size: 0.6em;">Builds to:</div>

        <pre>
$ docker image ls hwc
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hwc                 latest              142c29686b6a        15 hours ago        184 MB</pre>

        <aside class='notes'>
            <ul>
                <li>Here's a Dockerization of hello world, in C. By now, we should recognize the steps: we start from an operating system, use RUN to install dependencies, ADD to import files from our host machine, and define some default behavior with CMD.</li>
                <li>There's just one problem: we have successfully made a hello world application in a mere 184 MB. Giant images are at best slow to start, and can have security problems depending on what unnecessary components have been included.</li>
                <li>Most of this bloat is due to things we don't actually need in production: compilers, developer tools and the like.</li>
                <li>The Docker image builder implements Multi Stage Builds to allow you to create executables, then throw away the scaffolding needed to compile them, leaving you with a fast, lightweight image.</li>
            </ul>
        </aside>
    </section>    

    <section class="no_bg">
        <h2>Multi-Stage Builds</h2>

        <p>Hello World, lightweight:</p>

        <div class="row">
            <div class="col-6">
                <div class="pre"># Full SDK version (built and discarded)
FROM alpine:3.5 <span class="red-bg">AS build</span>
RUN apk update &amp;&amp; \
    apk add --update alpine-sdk
RUN mkdir /app
WORKDIR /app
ADD hello.c /app
RUN mkdir bin
RUN gcc -Wall hello.c -o bin/hello 

# Lightweight image returned as final product
FROM alpine:3.5
<span class="red-bg">COPY --from=build /app/bin/hello /app/hello</span>
CMD /app/hello</div>

            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre">FROM golang:nanoserver <span class="red-bg">as gobuild</span>
COPY . /code
WORKDIR /code
RUN go build hello.go

FROM microsoft/nanoserver
<span class="red-bg">COPY --from=gobuild /code/hello.exe /hello.exe</span>
EXPOSE 8080
CMD ["\\hello.exe"]</div>
            </div>
    -->
        </div>
        <br>
        <div style="font-size: 0.6em;">Builds to:</div>
        <div class="pre">$ docker image ls hwc
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
hwc                 latest              5d925cfc9c96        39 seconds ago      <span class="red-bg">4MB</span></div>

        <aside class='notes'>
            <ul>
                <li>To make a lightweight version of hello world with all the developer tools stripped out, we start with the exact same Dockerfile, but we've added the AS clause to the FROM statement.</li>
                <li>Then, we've added a second stanza, where we start from the same OS, but instead of installing the developer's kit, we use the --from flag with COPY to reference the 'build' image described above, and copy just the final executable over into our final image.</li>
                <li>The --from flag to COPY can also also specify an earlier image by index counting from 0 (so --from=0 would have had the same effect in the second stanza above).</li>
                <li>Note that it kind of looks like we built two images here - in fact, only the final FROM stanza results in an image on disk. All previous stanzas create cached image layers, but no final image.</li>
            </ul>
        </aside>        
    </section>

    <section class="no_bg">   
        <h2>Build Targets</h2>
        <p>Dockerfile</p>
        <div><pre>FROM &lt;base image&gt; as base
...

FROM &lt;foo image&gt; as foo
...

FROM &lt;bar image&gt; as bar
...

FROM alpine:3.4
...
COPY --from foo ...
COPY --from bar ...
...</pre></div>  
        <p>Building the image</p>
        <code>docker image build --target &lt;name&gt; ...</code>

        <aside class="notes">
            <ul>
                <li>We can also build intermediate images by specifying the "--target" parameter with the name of the intermediate build.</li>
                <li>If no "--target" is provided then the "docker image build" command always builds only the last image (the one starting with the last FROM statement in the Dockerfile)</li>
                <li>The &lt;name&gt; of an intermediate image is either the index of the FROM in the Dockerfile or the alias provided in the FROM statement (e.g. FROM base as test - in that case &lt;name&gt; would be "test")</li>
            </ul>
        </aside>
    </section>

   <!--
       <section class="no_bg">
        <h2>BuildKit</h2>

        <ul>
            <li>Speed-optimized builder, enable via <code>export DOCKER_BUILDKIT=1</code></li>
            <li>Parallelizes multi-stage builds</li>
            <li>Custom frontends</li>
            <li>2x - 9x build speedup</li>
            <li>Linux only as of 18.09.0-ee</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>BuildKit is an optional builder designed to make the image building process faster, better optimized, and more customizable.</li>
                <li>BuildKit parallelizes multistage builds and introduces a number of behind-the-scenes optimizations for creating a greater than 2x speedup in build times.</li>
                <li>BuildKit also supports the creation of custom frontends that allow new features and customizations to be more easily integrated into the build process.</li>
            </ul>
        </aside>

    </section>
   --> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Multi-Stage Builds</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='multi-stage-builds.md'>Multi-Stage Builds</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section>

    <section class="no_bg">
        <h2>Image Construction Best Practices</h2>

        <ul>
            <li>Start with official images</li>
            <li>Use multi-stage builds to drop compilers, SDKs...</li>
            <li>More layers leverage the cache...</li>
            <li>...but fewer layers perform better.</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Now that we have the mechanics of making Dockerfiles, there's also a number of optional best practices to consider.</li>
                <li>Base your images off of official images whenever possible; you can recognize these on Docker Hub as they don't have an explicit namespace like vendor/product; they're just single-word names, possibly with a tag. These are all battle-tested images produced in collaboration between the product vendors and Docker, and are scanned regularly for security vulnerabilities.</li>
                <li>Take advantage of multi-stage builds; these allow you to drop unnecessary layers, which will result in faster container start times, and less components that potentially inject vulnerabilities into your containers.</li>
                <li>Deciding how many layers to build an image out of depends on your priorities. The fundamental tension is that more layers leverage the cache better (since hopefully you don't invalidate the cache until you're most of the way through your Dockerfile), but this creates more overhead at container runtime, which you may wish to avoid for production images.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Development: More Layers</h2>

        <div class='row'>
            <div class='col-6'>
                <p>Bad caching:</p>
                <pre class='large'>
FROM python:3.5-alpine
RUN mkdir /app
<span class="red-bg">COPY /mypy /app/</span>
RUN pip install -r app/reqs.txt
...</pre>
            </div>
            <div class='col-6'>
                <p>Good caching:</p>
                <pre class='large'>
FROM python:3.5-alpine
RUN mkdir /app
<span class="red-bg">COPY /mypy/reqs.txt /app/</span>
RUN pip install -r app/reqs.txt
<span class="red-bg">COPY /mypy /app/</span>
...</pre>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>A common best practice during development is to split up oft-changing and rarely-changing elements into different layers. Move the rarely-changing parts as high as possible in the Dockerfile, so they don't have to be redone when the frequently changing parts are changed.</li>
                <li>In this case, we save ourselves from redoing the `pip install` when anything other than the requirements file changes.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Production: Less Layers</h2>

        <ul>
            <li>
                To collapse ALL image layers:
                <pre class='large'>
docker container run -d --name demo mytallimage:1.0
docker container export demo > image.tar
cat image.tar | docker image import - myflatimage:1.0</pre>
            </li>
            <li>Or build with <code>--squash</code> flag (experimental): compress all non-base layers</li>
            <li>Combine <code>container export</code> with <code>--squash</code> for one shareable base layer + one application-specific upper layer</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Once it's time to go to production (or even to start CI/CD), we don't care so much about build times and caching. The image is nominally built - what matters is performance.</li>
                <li>One way to compress everything into a single layer is to export a container as a tarball, and reimport it as a new, single layer image. This completely destroys the ability of containers to share layers, though</li>
                <li>Another method is the experimental squash flag, which combines all non-base layers into a single layer. Now the base layer remains sharable, and our production image is only two layers.</li>
                <li>One technique for getting the best of both worlds when layer sharing is important is to use the first method to collapse all widely shared layers into a common base image, and then use the --squash flag on subsequent builds to squash the application-unique layers into a single application layer.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Best Practice: Patching &amp; Updates</h2>

        <img src="src/modules/Training-for-Containerization/04-creating-images/images/patching.png" title="Patching and Updates" class="transparent" style='width:60%'>
        <aside class="notes">
            <ul>
                <li>When revving an image, don't just apply patches on top of old images. If it's your base layer that's been revved, the vendor will likely release a new image corresponding to the new software version; update your Dockerfile and rebuild your image with the new base layer.</li>
                <li>The same logic holds true for updating other image layers; rebuild your image from its Dockerfile, pulling in the desired versions of your dependencies, rather than just installing patches on top of patches like you would for software installed on the host.</li>
                <li>Remember copy on write: when you apply a patch, it doesn't overwrite whatever its upgrading; all versions of all files are persisted in their entirety in an ever-growing image layer stack. This will bloat your images and slow down their performance.</li>
            </ul>
        </aside>
     </section>

    <section class="no_bg">
        <h2>Image Tags</h2>

        <ul>
            <li>Optional string after image name, separated by <code>:</code></li>
            <li><code>:latest</code> by default</li>
            <li>Same image with two tags shares same ID, image layers:</li>
        </ul>

        <pre>
$ docker image ls centos*
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
centos              7                   8140d0c64310        7 days ago          193 MB
$ docker image tag centos:7 centos:mytag
$ docker image ls centos*
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
centos              7                   8140d0c64310        7 days ago          193 MB
centos              mytag               8140d0c64310        7 days ago          193 MB</pre>

        <aside class='notes'>
            <ul>
                <li>In addition to the name of the image, images can be given an optional tag.</li>
                <li>Tags are often used to capture version number or base image distro.</li>
                <li>The tag will default to `latest` if omitted.</li>
                <li>Note that tags are essentially just pointers to an image which is uniquely identified by its ID; creating another tag pointing to the same image doesn't duplicate the image on disk, but just creates another reference to it.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Image Namespaces</h2>
        <p>Images exist in one of three namespaces:</p>
        <ul>
            <li>Root (<code>ubuntu</code>, <code>nginx</code>, <code>mongo</code>, <code>mysql</code>, ...)</li>
            <li>
                User / Org (<code>jdoe/myapp:1.1</code>, 
                <code>microsoft/nanoserver:latest</code>, ...)
            </li>
            <li>Registry (<code>FQDN/jdoe/myapp:1.1</code>, ...)</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Certified images produced in collaboration between Docker and third-party software vendors are given single-word names in the root namespace.</li>
                <li>Images meant to be shared on hub.docker.com are namespaced via the owning account, then the image name</li>
                <li>Images stored in docker trusted registry are similar to hub.docker.com names, but prefixed with the FQDN of the registry.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Image Tagging &amp; Namespacing</h2>

        <ul>
            <li>Tag on build:<br>
                <div class="pre">docker image build -t myapp<span class="red-bg">:1.0</span> .</div>
            </li>
            <li>Retag an existing image:<br>
                <div class="pre">docker image tag myapp:1.0 <span class="red-bg">me/myapp:2.0</span></div>
            </li>
            <li>Note <code>docker image tag</code> can set both tag and namespace.</li>
            <li>Names and tags are just pointers to image ID</li>
            <li>Image ID corresponds to immutable content addressable storage</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Images can be tagged on build or retagged at any time.</li>
                <li>Note that image layers are only stored once per machine; retagging or renaming an image does not duplicate the image layer.</li>
                <li>Always remember that an image must be namespaced correctly to push to a registry, whether it's hub.docker.com or Docker Trusted Registry.</li>
                <li>Finally, remember that docker registries all use content addressable storage models; image names and tags are really just human-friendly pointers to image IDs, which serve as the true address for immutable image information. As such, it is a good security strategy to pull by sha and not by tag; then you always know exactly what you're getting.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Sharing Images</h2>

        <ul>
            <li>
                Docker Hub
                <ul>
                    <li>Provides certified commercial and free software distributed as Docker Images</li>
                    <li>Shares community-generated images and content</li>
                </ul>
            </li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Docker Hub allows you to access and share your public repositories with the Docker community at large. You can download two types of images from the Docker Hub: Docker Verified Images and Community/Hub images.</li>
                <li>Docker Hub is a cloud-based registry service which allows you to link to code repositories, build your images and test them, stores manually pushed images, and links to Docker Cloud so you can deploy images to your hosts. If you have built images, you can push them to a Docker Hub repository that you add to your Docker Hub user or organization account.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/04-creating-images/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Managing Images</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='managing-images.md'>Managing Images</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Image Creation Takeaways</h2>

        <ul>
            <li>Images are built out of read-only layers.</li>
            <li>Dockerfiles specify image layer contents.</li>
            <li>Key Dockerfile commands: <code>FROM</code>, <code>RUN</code>, <code>COPY</code> and <code>ENTRYPOINT</code></li>
            <li>Images must be namespaced according to where you intend on sharing them.</li>
        </ul>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Best practices for writing Dockerfiles: <a href="http://dockr.ly/22WiJiO">http://dockr.ly/22WiJiO</a></li>
            <li>Use multi-stage builds: <a href="http://dockr.ly/2ewcUY3">http://dockr.ly/2ewcUY3</a></li>
            <li>More about images, containers, and storage drivers: <a href="http://dockr.ly/1TuWndC">http://dockr.ly/1TuWndC</a></li>
            <li>Details on image layering: <a href='https://bit.ly/2AHX7iW'>https://bit.ly/2AHX7iW</a></li>
            <li>Graphdriver plugins: <a href="http://dockr.ly/2eIVCab">http://dockr.ly/2eIVCab</a></li>
            <li>Docker Reference Architecture: An Intro to Storage Solutions for Docker CaaS: <a href="http://dockr.ly/2x8sBw2">http://dockr.ly/2x8sBw2</a></li>
            <li>How to select a storage driver: <a href="http://dockr.ly/2eDu8yO">http://dockr.ly/2eDu8yO</a></li>
            <li>Use the AUFS storage driver: <a href="http://dockr.ly/2jVc1Zz">http://dockr.ly/2jVc1Zz</a></li>
            <li>User guided caching in Docker: <a href="http://dockr.ly/2xKafPf">http://dockr.ly/2xKafPf</a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>additional resources about creating images for Linux</li>
            </ul>
        </aside>
    </section>

    <!--
    <section class="no_bg">
        <h2>Further Reading 2/2</h2>
        <h3>Windows Containers:</h3>
        <ul>
            <li>Dockerfile on Windows: <a href="http://bit.ly/2waNvsS"><span class="link">http://bit.ly/2waNvsS</span></a></li>
            <li>Optimize Windows Dockerfiles: <a href="http://bit.ly/2whpfn7"><span class="link">http://bit.ly/2whpfn7</span></a></li>
            <li>Windows Container Samples:
                <ul>
                    <li> <a href="http://bit.ly/2wCrPXy"><span class="link">http://bit.ly/2wCrPXy</span></a></li>
                    <li> <a href="http://bit.ly/2ghRr5o"><span class="link">http://bit.ly/2ghRr5o</span></a></li>
                </ul>
            </li>
            <li>Powershell Tricks: <a href="http://bit.ly/2wb7Azn"><span class="link">http://bit.ly/2wb7Azn</span></a></li>
            <li>Multi-stage builds for Windows containers: <a href="http://bit.ly/2iBRmdN"><span class="link">http://bit.ly/2iBRmdN</span></a></li>
            <li>The SHELL command: <a href="http://dockr.ly/2whvyqZ"><span class="link">http://dockr.ly/2whvyqZ</span></a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>additional resources about creating images for Windows</li>
            </ul>
        </aside>
    </section>
    -->
</section>
<section>
    <section class="no_bg">
        <h2>Docker Volumes</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Managing Data</h2>

        <p>If a container generates a lot of data, where should it be stored? What if you need to provision data to a container?</p>

        <aside class='notes'>
            <ul>
                <li>Guide the class to thinking about the fact that so far, containers and images don't provide a practical way to manage data that lives longer than the lifetime of a container.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Should you write a lot of data to the container layer? That's currently the only place we've learned about in this workshop where you can write data to at container run time. Why wouldn't you want to do this?</li>
                <li>Should you provision data to a container by including that data in the underlying image? Again, this is the only option we've seen so far. Why wouldn't you want to do this?</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Define a volume and identify its primary use cases</li>
            <li>Describe the advantages and potential security risks of mounting volumes and host directories into containers</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>Volume Usecases</h2>

        <p>Volumes provide a R/W path <span class='keyword'>separate from the layered filesystem</span>.</p>

        <ul>
            <li><span class='keyword'>Mount</span> data at container startup</li>
            <li><span class='keyword'>Persist</span> data when a container is deleted</li>
            <li><span class='keyword'>Share</span> data between containers</li>
            <li><span class='keyword'>Speed up</span> I/O by circumventing the union filesystem</li>
        </ul>

        <aside class="notes">
            <ul>
                <li>Volumes primarily provide a way to handle data that has a longer lifecycle than an individual container, by providing a writable location separate from the container's union filesystem.</li>
                <li>For example, if a container needs access to a large body of files, those files can be mounted into a running container as a volume, avoiding the need to create a (potentially huge) image with that data baked in.</li>
                <li>If a container is creating or collecting data as it runs, it should be stored in a volume, since that volume will survive the deletion of the container.</li>
                <li>Furthermore, volumes can be a more performant choice for write heavy workloads for the same reason. Rather than searching the layers of the union filesystem and performing a copy on write operation when writing a file, I/O in a volume simply reads and writes the relevant file, without the added overhead.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Basic Volumes</h2>

        <ul>
            <li><span class='keyword'>Named</span>: managed by Docker; filesystem independent; user-specified identifier</li>
            <li><span class='keyword'>Anonymous</span>: managed by Docker; filesystem independent; randomly-generated identifier</li>
            <li><span class='keyword'>Host mounted</span>: mount a specific path on the host; DIY management</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>There are two high-level methods for mounting volumes into containers: named volumes, and host mounts.</li>
                <li>In the named volume case, Docker will create and manage a directory on your host for your volume, using the 'docker volume' CLI commands.</li>
                <li>Host mounts specify a particular path on the host to mount into a container. This is useful if there's something host specific you want to provide to the container, but requires you to manage the host's filesystem directly and depends on the path in question actually being meaningful on that host, potentially affecting portability.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/05-volumes/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Volumes</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='basic-volume-usage-demo.md'>Basic Volume Usage</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section class="no_bg">
        <h2>Volumes in dockerfiles</h2>
        <ul>
            <li>VOLUME instruction creates a mount point</li>
            <li>Can specify arguments in a JSON array or string</li>
            <li>Cannot map volumes to host directories</li>
            <li>Volumes are initialized when the container is executed</li>
        </ul>
        <br>
        <br>
        <div class="row">
            <div class="col-6">
                <div class="pre">FROM nginx:latest
...
# string example
VOLUME /myvolume

# string example with multiple volumes
VOLUME /www/website1 /www/website2

# JSON example
VOLUME ["myvol1", "myvol2"]
...</div>
            </div>
            <!--
            <div class="col-6">
                Windows containers:<br>
                <div class="pre">FROM microsoft/iis:nanoserver
...
# string examples
VOLUME c:\\data
VOLUME c:\\data2 c:\\data3

# JSON examples
VOLUME ["c:\\\\data4", "c:\\\\data5"]
VOLUME ["c:/data6", "c:/data7"]
...</div>
            </div>
        -->
        </div>
        <aside class="notes">
            <ul>
                <li>VOLUME is another Dockerfile instruction available to designate a directory inside a container as a volume to be persisted on the host.</li>
                <li>Note that this syntax does not allow for the specification of a host path, since that would break our all-important portability; there's no guarantee that path will exist on any arbitrary machine in a meaningful way.</li>
                <li>Docker automatically creates a volume (directory) on the host for each volume that is declared in the Dockerfile. The name of such a host volume is a sha256. The data that is then stored inside those volumes inside the container is mapped/persisted to the host FS.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Volumes and Security</h2>

        <ul>
            <li>Point of ingress to the host and other containers</li>
            <li>Don't mount things unnecessarily</li>
            <li>Use the <code>:ro</code> flag</li>
            <li>Linux: in-memory <code>tmpfs</code> mounts available</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Volumes are the first thing we've seen so far that pierce the isolation we carefully crafted between containers, the host, and other containers. A malicious actor can inject files from one container to another, and to the host, if care is not taken.</li>
                <li>Mount volumes and directories only as needed, and use the :ro flag when a container is only a passive consumer of data. Better yet, ask if there's a way to separate out reader and writer containers, to keep write access to volumes as tightly restricted as possible.</li>
                <li>Also for those running linux hosts, it's also possible to mount purely in-memory volumes to a container; anything written here will never be written to disk, and will be released when the container is deleted. This is a good option for persisting sensitive data while a container is running.</li>
            </ul>
        </aside>

    </section>


    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/05-volumes/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Volumes Usecase</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='database-volumes.md'>Database Volumes</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>

    </section> 

    <section class="no_bg">
        <h2>Docker Volume Takeaways</h2>
        <ul>
            <li>Volumes persist data beyond the container lifecycle</li>
            <li>Volumes bypass the copy on write system (better for write-heavy containers)</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>The most important take home message for volumes, is that this is where persistent data should go - not in containers, which come and go rapidly.</li>
                <li>Also, volumes are separate from the union file system; changes to volumes do not precipitate changes to images, and vice versa.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>How to use volumes: <a href="http://dockr.ly/2vRZBDG">http://dockr.ly/2vRZBDG</a></li>
            <li>Troubleshoot volume errors: <a href="http://dockr.ly/2vyjvbP">http://dockr.ly/2vyjvbP</a></li>
            <li>Docker volume reference: <a href="http://dockr.ly/2ewrlew">http://dockr.ly/2ewrlew</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about volumes</li>
            </ul>
            
        </aside>
    </section>

</section>
<section>
    <section class="no_bg">
        <h2>Wrap Up 1 - Spring Boot</h2>
    </section>
    <section data-background="#00a2a1" class="green_bg">
        <h2>Exercise Instructions</h2>
        <ul>
            <li>Goal: Build a docker image that runs a Java application</li>
            <li>Clone the git repository at <a href="https://github.com/ckaserer/java-helloworld">https://github.com/ckaserer/java-helloworld</a></li>
            <li>Build the application using <code>gradle bootJar</code> and find the resulting fat-jar file at <code>build/libs</code> (already done for you).</li>
            <li>You can run the java application locally with <code>java âjar rest-service-0.0.1-SNAPSHOT.jar</code>, this starts an application server on port 8080. Check it in your browser via <a href="http://127.0.0.1:8080/greeting">http://127.0.0.1:8080/greeting</a> (respectively with your AWS instance public dns).</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
    </section>
    
    <section data-background="#00a2a1" class="green_bg">
        <h2>Solution Hints</h2>
        <ul>
            <li>Build the application outside of the container (already done and checked into git).</li>
            <li>Find a suitable docker base image, that has a JRE installation.</li>
            <li>Use both the <code>CMD</code> and the <code>ENTRYPOINT</code> instructions.</li>
            <li>Run two instances of the container â what do you have to take care of? Make sure both instances are reachable via browser.</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
    </section>
    
    <section class="no_bg">
        <h2>Solution</h2>
        <p>Sample Dockerfile</p>
        <div class='pre'>FROM anapsix/alpine-java
LABEL MAINTAINER=clemens.kaserer@gepardec.com
WORKDIR /data
EXPOSE 8080
COPY build/libs/rest-service-0.0.1-SNAPSHOT.jar \
     rest-service-0.0.1-SNAPSHOT.jar
CMD ["-jar", "rest-service-0.0.1-SNAPSHOT.jar"]
ENTRYPOINT ["java"]</div>            
        <p>Solution Commands</p>
            <ul>
                <li>docker buildÂ -t spring_boot_example .</li>
                <li>docker run âd âp 80:8080 spring_boot_example</li>
            </ul>

    </section>
</section><section class="no_bg">
    <h2>Containerization Fundamentals Conclusion: Any App, Anywhere.</h2>

    <ul>
        <li>Containers are isolated processes</li>
        <li>Images provide filesystem for containers</li>
        <li>Volumes persist data</li>
    </ul>

    <aside class='notes'>
        <ul>
            <li>the key takehome from basic containerization is an understanding of how it lets docker deliver on its promise to enable you to run any app, anywhere.</li>
            <li>the layered filesystems defined by Dockerfiles which in turn define images contain all the execution context a process needs to run; features of the linux kernel like kernel namespacing and control groups allow that environment to be created as a container on any host linux system, securely and without regard to whatever else is running on that machine. These tools provide the standardization and encapsulation we predicted would be of benefit in the introduction at the start of the day.</li>
            <li>One slightly subtler point is that in none of this did we ever impose any restrictions on how many containers could be running on a given machine, or any necessary connections between the images that underlie them. This ability to mix and match frees us from correlations between processes; run your postgres database from ubuntu, your node.js web app from debian and your FORTRAN data wrangling from centos if you want - all on the same machine. Containerization's implicit win is the ability to always use the right tool for the job.</li>
        </ul>
    </aside>

</section>

<section>
    <section class="no_bg">
        <h2>Docker Networking Basics</h2>

        <aside class="notes">
            <ul>
                <li>[Instructor aside: this module is intended to bridge between one day of introduction to containerization, and one day of introduction to orchestration. As such it can equally well go at the end of the first or the beginning of the second, or be omitted entirely if the workshop isn't introducing orchestration].</li>
                <li>To begin our exploration of orchestration, we first need to examine the basics of how two containers can be networked together on a single host.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Portable Networks</h2>

        <p>Network traffic must by definition traverse a network outside its originating container. How can we make inter-container communication as portable and secure as containers themselves?</p>

        <aside class='notes'>
            <ul>
                <li>Lead the class to think about the need for networking abstractions, both software defined networks and DNS-resolvable container names.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>Can we rely on a container having the same IP or mac address every time it is created, no matter what host it is created on? (obviously not, a given private IP could already be taken on a destination host, and any global public identifier like a public IP or mac address couldn't be practically registered at container run time). Therefore, we must need some sort of networking layer that abstracts away the host network. That also might give us the opportunity to impose some security on our networks, since we control this extra networking layer.</li>
                <li>How will service discovery work in our containerized application logic? We'd like to avoid having a lot of boilerplate that exposes the networking underlay to our application logic; Docker should provide some portable addressing mechanism. (leads to thinking about DNS).</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Describe Docker's container network model and its security implications</li>
            <li>Describe the basic technologies that underwrite single host networks</li>
            <li>Understand how Docker manipulates a host's firewall rules to control container traffic</li>
        </ul>
    </section>

    <section class="no_bg">
        <h2>The Container Network Model</h2>

        <img src='src/modules/Training-for-Containerization/07-networking-basics/images/cnm-simplified.png'></img>

        <aside class='notes'>
            <ul>
                <li>At high level, docker thinks about networking with an abstraction called the Container Network Model (CNM) that consists of 3 parts:</li>
                <li>The container (network) sandbox, which firewalls containers by default.</li>
                <li>The network endpoint, which serves as a controlled port in and out of the container sandbox</li>
                <li>The network itself, which is any device that facilitates inter-container communication.</li>
                <li>If you think about it for a moment, the CNM is in some sense very vague; anything that satisfies these requirements is a valid implementation option. Just like we saw with containerization itself, Docker leverages battle-tested kernel features and linux tools to realize the CNM in practice.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: Default Single-Host Network</h2>

        <img src='src/modules/Training-for-Containerization/07-networking-basics/images/default-network-1.png'></img>

        <aside class='notes'>
            <ul>
                <li>When Docker is started on the host, a linux bridge is created by default, and assigned an unused private subnet from 172.[17-31].0.0/16 or 192.168.[0-240].0/20. A linux bridge is an in-software switch, that routes packets by MAC address.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: Default Container Networking</h2>

        <img src='src/modules/Training-for-Containerization/07-networking-basics/images/default-network-2.png'></img>
        <p><span class='keyword'>Quiz:</span> identify the sandbox, endpoint and network corresponding to the container networking model objects in this diagram.</p>

        <aside class='notes'>
            <ul>
                <li>When a container is created, the container networking model must be satisfied. Containers run in their own network namespace, satisfying the sandboxing requirement; inside the namespace, processes won't be able to access host networking devices unless explicitly connected to them.</li>
                <li>In order to connect a container to the rest of the system, a virtual ethernet (veth) pair is created, with one endpoint connected to the default docker bridge, and the other presented as an ethX port inside the container with a private IP taken from the bridge's subnet. Veth connections operate as a pipe, forwarding all traffic in one end to the other, even across network namespaces.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Linux: User-Defined Bridges &amp; Firewalls</h2>

        <img src='src/modules/Training-for-Containerization/07-networking-basics/images/user-bridge.png'></img>

        <aside class='notes'>
            <ul>
                <li>Users can optionally create their own linux bridges, and plug containers into them for communicating on the same host.</li>
                <li>Containers on user-created bridge networks can resolve each other by container name; containers names are registered in the Docker daemon's DNS table for resolution.</li>
                <li>You'll build something like this in the next exercise.</li>
            </ul>
        </aside>
    </section>
<!--
    <section class="no_bg">
        <h2>Windows: The nat Network</h2>

        <img src='src/modules/Training-for-Containerization/07-networking-basics/images/win-nat.png'></img>

        <aside class='notes'>
            <ul>
                <li>While the windows networking stack is completely different, many of its components follow a close analogy to the linux stack; both use network namespaces, for example, to satisfy the sandboxing requirement of the docker network model.</li>
                <li>Windows hosts use a Hyper-V virtual switch combined with the WinNAT network address translation service to provide the same layer-2 routing that a linux bridge does; this serves the role of the docker software-defined network in the network model.</li>
                <li>Endpoints in windows are provided by ports in the virtual switch connected to virtual NICs in windows server containers, or VM NICs inside hyper-v containers.</li>
                <li>Windows Firewall rules provide similar constraints on container communication as in linux.</li>
                <li>Note that the default windows NAT network is deleted automatically on reboot in Windows Server 2019; Docker will recreate a NAT network for itself, but any containers plugged into a previous NAT will get new IPs after a reboot.</li>
            </ul>
        </aside>
    </section>
-->
    <section class="no_bg">
        <h2>Exposing Container Ports</h2>

        <ul>
            <li>Containers have no public IP address by default.</li>
            <li>Can forward host port -> container port</li>
            <li>Mapping created manually or automatically.</li>
            <li>Port mappings visible via <br><code>docker container ls</code> or <br><code>docker container port</code></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>An axiom of Docker security can be thought of as 'isolated by default'</li>
                <li>In terms of networking, this implies that containers are not reachable from the outside world by default.</li>
                <li>Port mappings have to be set up if a containerized process is to be reachable directly.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/07-networking-basics/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Single Host Networks</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='single-host-network-demo.md'>Single Host Networks</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/07-networking-basics/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Single Host Networks</h2>
        <p>Work through</p>

        <ul>
            <li class='exercise' script='introduction-to-container-networking.md'>Introduction to Container Networking</li>
            <li class='exercise' script='container-port-mapping.md'>Container Port Mapping</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Docker Networking Takeaways</h2>

        <ul>
            <li>
                Single host networks follow the container networking model:
                <ul>
                    <li>Sandbox: Network namespaces</li>
                    <li>Endpoint: veth (linux)</li>
                    <li>Network: bridge (linux)</li>
                </ul>
            </li>
            <li>Containers resolve each other by DNS lookup when explicitly named and attached to custom networks</li>
            <li>Docker software defined networks are firewalled from each other by default</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>The key takeaway for Docker container networking is the paradigm of isolation by default. Containers must be explicitly connected to the same network to talk to each other; leverage this to easily improve the security of your applications.</li>
                <li>Similarly, containers are not exposed on the external network by default; they must explicitly have ports mapped to the host if they are to be reachable by the outside world. Do not expose or map ports unnecessarily, as this leads to port conflicts and security risks!</li>
                <li>For much more detail, see the corresponding reference architecture linked below.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Docker Reference Architecture: Designing Scalable, Portable Docker Container Networks: <a href="https://dockr.ly/2q3O8jq">https://dockr.ly/2q3O8jq</a></li>
            <li>Network containers: <a href="http://dockr.ly/2x1BYgW">http://dockr.ly/2x1BYgW</a></li>
            <li>Docker container networking: <a href="http://dockr.ly/1QnT6y8">http://dockr.ly/1QnT6y8</a></li>
            <li>Understand container communication: <a href="http://dockr.ly/2iSrHO0">http://dockr.ly/2iSrHO0</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about networking basics</li>
            </ul>
            
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Introduction to Kubernetes</h2>
    </section>

    <section class="no_bg">
        <h2>Discussion: Any App, Anywhere?</h2>

        <p>Containers are portable. What does this imply about the best ways to manage a containerized data center?</p>

        <aside class='notes'>
            <ul>
                <li>Lead class to consider the advantage of letting containers get scheduled anywhere, rather than controlling exactly where they get scheduled. Then, explore what the consequences of what spreading containers across multiple hosts are (need for management, control and data planes; need for some special-case control over scheduling decisions).</li>
                <li>Hint questions if the class is stuck:</li>
                <li>In general, does it matter where a given container gets scheduled? Why or why not? (conclude that in most cases, it doesn't matter, but there may be special cases such as needing specific hardware, or being sensitive to resource availability).</li>
                <li>If we're scheduling containers on any arbitrary host in our datacenter, we're going to need some sort of cluster manager in order to administrate all these hosts and containers. How will we provide service discovery? (Consider things like peer-to-peer service discovery (no choke points but could generate a lot of traffic), hub and spoke distribution (potentially less traffic but single points of failure), lookup from a kv store (just-in-time lookup could minimize traffic but might block traffic depending on implementation). Others?)</li>
                <li>Once we've solved the service discovery problem, how are we going to get packets from one host to another? Recall that so far, we've only seen linux bridges moving packets around at layer 2.</li>
                <li>Enabling this full-datacenter scheduling and solving the corresponding and communication challenges are the fundamental responsibilities of any production-ready container orchestrator.</li>
            </ul>
        </aside>
    </section>
    
    <!--
    <section class="no_bg">
        <h2>Discussion: Container Communication</h2>

        <p>Suppose you have two services you want to guarantee can reach each other on the same host, to eliminate network latency. How would you do this in Swarm?</p>

        <aside class='notes'>
            <ul>
                <li>Concrete example: an API tier that needs to hit a database tier - don't want to wait for network latency between the two.</li>
                <li>Entertain suggestions from the class for a while until concluding that there's no way to do this with swarm services. Kubernetes solves the same fundamental problems as any orchestrator, but gives an alternative networking and scheduling model that supports different use cases than Swarm.</li>
            </ul>
        </aside>

    </section>
    -->

    <section class="no_bg">
        <h2>Learning Objectives</h2>
        
        <p>By the end of this module, learners will be able to</p>

        <ul>
            <li>Understand the components and roles of Kubernetes masters and nodes</li>
            <li>Identify and explain the core Kubernetes objects (Pod, ReplicaSet, Deployment, Service, Volumes)</li>
            <li>Provision configuration via configMaps and secrets</li>
            <li>Explore the Kubernetes networking model</li>
        </ul>
    </section> 

    <section class="no_bg">
        <h2>Orchestrator Goals</h2>

        <p><span class='keyword'>Top-line goal:</span> operate a datacenter like a <span class='keyword'>pool of compute resources</span> (not individual machines). This requires</p>

        <ul>
            <li>Add / remove compute resources securely and easily</li>
            <li>Schedule containers across the cluster transparently</li>
            <li>Streamline container-to-container communication (service discovery, load balancing and routing)</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>The core goal of any orchestrator is to let you treat a bunch of computers like a pool of compute resources ready to support your containerized workloads; the fact that containers are so portable and work the same way anywhere means we generally don't have to care too much what specific node they get scheduled on. Instead, we'd like to delegate that decision to our orchestrator, and think about our cluster as a whole rather than as a lot of individual machines.</li>
                <li>In order to actually do that, there are some minimal requirements any orchestrator will need to satisfy:</li>
                <li>Not only do we have to be able to add new nodes to our cluster, this has to be done in a way that is secure at join time, and secure in communication long term.</li>
                <li>Our orchestrator will need to be able to schedule workload anywhere in the cluster; this is perhaps the easiest part, thanks to the portability of containers. Beyond this, our orchestrator should also be responsible for automatically maintaining that workload as much as possible.</li>
                <li>Finally, with our applications potentially divided among many containers and hosts, we'll need our orchestrator to help us communicate between these containers, by facilitating service discovery, load balancing and routing appropriate to the networking needs of each of our applications.</li>
                <li>There are of course many more things an orchestrator could do for us, but these are the core concerns that any feasible orchestrator must do.</li>
            </ul>
        </aside>
    </section>
    <section class="no_bg">
        <h2>Pets Versus Livestock</h2>

        <div class='row'>
            <div class='col-4'>
                <ul>
                    <li>Kubenetes reschedules exited containers automatically</li>
                    <li style='margin-right:1em'>When a container becomes unhealthy, <span class='keyword'>kill it and get a new one</span>.</li>
                </ul>
            </div>
            <div class='col-8'>
                <img src='src/modules/Training-for-Containerization/09-kubernetes/images/petsvslivestock.png' style="background:none !important;">
                 <figcaption style="font-size:50%; line-height:0; text-align:center; margin-top: 1em;">Dog photo <a href="https://www.flickr.com/photos/jeffreyww/4975374886/in/photolist-8zE6BC-c9tWCb-c9tXfd-5a7MyF-c3KLuL-5dDirY-axQTra-c9sgSG-amLcYQ-c3KFn7-c9tXo1-c9tWc9-dHZUz-rgAhXo-rW2iU1-KSM6e-pSCtLH-qF7ff3-5Sqz9E-fmSgbs-7KxKak-8TZKk8-6gVmH9-ehfejV-ehtrx4-bD41t2-aiMDUn-8U3iS9-DacH2L-63pHfi-8TZfEH-93nazZ-dmdgPv-85pGjL-8p9Un7-8p9Ui3-5a7EUt-8qP6BK-345kg4-8BRoqc-65p6Gq-YkNMy4-8J4rLP-o2EFGs-H43vF2-8U3jm3-8U48Fb-Si8zjf-8U48pY-2dBGh2">jeffreyw</a>; Livestock photo <a href="https://www.flickr.com/photos/pauljill/28350728097/in/photolist-KcfTJR-a38m8q-tNhS3j-27TRuXP-Ssmqi7-9uD1Xw-PyTy8A-4D77Jm-8pSu5Q-W3FcwF-ZypKkz-Ue3XvX-VP9jfs-28pSeWb-UP2ryi-LQt3rp-UT391d-MEqR17-Y7sweh-8X7vrU-Y7FJfm-QBydX8-FgTo81-XZr5cN-SPXUwJ-rhVK-bmDYEB-dPFa64-287ET1L-UZyj2p-TcrWzo-ZzPMvn-8JAYv1-48S7W5-8C3qEJ-J6FjAR-urgqiC-f4E4kU-DTLaj9-24prtP3-6P3yWW-28ynA9Q-UPjBk8-UyDxAW-XwKZRW-24HpVUY-ehvBKf-HbXbmw-BSCQ2s-8Uz7pW"> Paul Asman, Jill Lenoble</a>; images <a href='https://creativecommons.org/licenses/by/2.0/'>CC-BY 2.0</a></figcaption>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>In the exercises and demos so far, we've seen how Swarm will reschedule containers after they exit. This is a crucial feature for how we think about managing orchestrated container workloads.</li>
                <li>We often describe this mindset as livestock management, versus pet care. With a pet, we concern ourselves with every injury and illness, and attempt to keep our pet healthy and happy as long as possible; this is the wrong way to think about troubleshooting containers. As any rancher knows, what matters is not the health of an individual livestock animal, but the health of the herd. If one animal gets sick, the right course of action is to kill it and get a new one. The same is true with containers; if a container scheduled by swarm misbehaves, the first course of action to take is to kill it and let Swarm reschedule it.</li>
                <li>If pathologies persist after rescheduling fresh containers, then a more serious debugging effort makes sense - but not when a single container gets rescheduled or has to be restarted.</li>
                <li>Bear in mind this is usually a big change from how we thought about managing VMs - VMs were more like pets we want to keep alive. Bringing the same management and troubleshooting mindset to the containerization world is a common mistake that causes new container users a lot of unnecessary pain.</li>
            </ul>
        </aside>
    </section> 

    <section class="no_bg">
        <h2>Kubernetes Master</h2>
        <div class="row">
            <div class="col-7">
                <p>Important Components</p>
                <ul>
                    <li><span class="keyword">API Server:</span> Frontend into Kubernetes control plane</li>
                    <li><span class="keyword">Cluster Store:</span> Config and state of cluster</li>
                    <li><span class="keyword">Controller Manager:</span> Assert desired state</li>
                    <li><span class="keyword">Scheduler:</span> Assigns workload to nodes</li>
                </ul>
            </div>
            <div class="col-5">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/master.png" title="master" style="margin-left: 20px;">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>The API Server (apiserver) is the frontend into the Kubernetes control plane. It exposes a RESTful API that preferentially consumes JSON. We POST manifest files to it, these get validated, and the work they define gets deployed to the cluster.</li>
                <li>The config and state of the cluster gets persistently stored in the cluster store, which is the only stateful component of the cluster and is vital to its operation - no cluster store, no cluster!<br> The cluster store is based on etcd, the popular distributed, consistent and watchable key-value store. As it is the single source of truth for the cluster, you should take care to protect it and provide adequate ways to recover it if things go wrong.</li>
                <li>The controller manager (kube-controller-manager) implements things like the node controller, endpoints controller, namespace controller etc. They tend to sit in loops and watch for changes â the aim of the game is to make sure the current state of the cluster matches the desired state</li>
                <li>At a high level, the scheduler (kube-scheduler) watches for new workloads and assigns them to nodes. Behind the scenes, it does a lot of related tasks such as evaluating affinity and anti-affinity, constraints, and resource management.</li>
                <li>Note that directly analogous components appear in a Swarm manager; in Swarm's case they are integrated directly into the Docker engine, but the same roles and responsibilities appear in both. This is the first example of what we meant by Kube being more modularly designed.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Node</h2>
        <div class="row">
            <div class="col-7">
                <ul>
                    <li><span class="keyword">Kubelet:</span> Kubernetes Agent</li>
                    <li><span class="keyword">Container Engine:</span> Host Containers</li>
                    <li><span class="keyword">Network Proxy:</span> Networking &amp; Load Balancing</li>
                </ul>
            </div>
            <div class="col-5">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/node.png" title="master" style="margin-left: 20px;">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Kubelet: This is the main Kubernetes agent that runs on all cluster nodes. When kubelet is installed on a Linux host then it registers the host with the cluster as a node. It then watches the API server for new work assignments. Any time it sees one, it carries out the task and maintains a reporting channel back to the master.</li>
                <li>If the kubelet canât run a particular work task, it reports back to the master and lets the control plane decide what actions to take. For example, if a Pod fails on a node, the kubelet is not responsible for restarting it or finding another node to run it on. It simply reports back to the master. The master then decides what to do.</li>
                <li>Container Engine: The Kubelet needs to work with a container runtime to do all the container management stuff â things like pulling images and starting and stopping containers. More often than not, the container runtime that Kubernetes uses is Docker. In the case of Docker, Kubernetes talks natively to the Docker Remote API.</li>
                <li>More recently, Kubernetes has released the Container Runtime Interface (CRI). This is an abstraction layer for external (3rd-party) container runtimes to plug in to. Basically, the CRI masks the internal machinery of Kubernetes and exposes a clean documented container runtime interface. The CRI is now the default method for container runtimes to plug-in to Kubernetes. The containerd CRI project is a community-based open-source project porting the CNCF containerd runtime to the CRI interface.</li>
                <li>Kube Proxy: The last piece of the puzzle is the kube-proxy. This is like the network brains of the node. For one thing, it makes sure that every Pod gets its own unique IP address. It also does lightweight load-balancing on the node.</li>
            </ul>
        </aside>
    </section>
        
    <section class="no_bg">
        <h2>Architecture</h2>
        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/architecture.png" title="Architecture">
        <aside class="notes">
            <ul>
                <li>In this schema we have one master and 3 nodes. On the master we find the 4 services that we discussed earlier: API service, scheduler, controller manager and cluster storage</li>
                <li>On each of the nodes we have kubelet, Kubernetes proxy and the container host such as containerd. The container host runs containers C1, ..., Cx</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Orchestration Objects</h2>

        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/deployment.png" title="Deployment">

        <aside class='notes'>
            <ul>
                <li>While Docker Swarm provides a complete orchestration stack in its service objects, Kubernetes defines several different entities that must be combined to achieve the same result. This is the other way in which kubernetes is more modular than swarm, in that its orchestration abstractions are broken down into multiple objects.</li>
                <li>Pod: In Kubernetes a Pod is the atomic unit of scheduling. We cannot run containers directly on a Kubernetes cluster.</li>
                <li>At the highest-level, a Pod is a ring-fenced environment to run containers. The Pod itself doesnât actually run anything, it's just a sandbox to run containers in. Keeping it high level, we ring-fence an area of the host OS, build a network stack, create a bunch of kernel namespaces, and run one or more containers in it.</li>
                <li>If one runs multiple containers in a Pod, they all share the same environment - things like the IPC namespace, shared memory, volumes, network stack etc. As an example, this means that all containers in the same Pod will share the same IP address (the Podâs IP).</li>
                <li>ReplicaSet: A ReplicaSet is a higher-level Kubernetes object that wraps around a Pod and adds features. As the names suggests, they take a Pod template and deploy a desired number of replicas of it. They also instantiate a background reconciliation loop that checks to make sure the right number of replicas are always running â desired state vs actual state. ReplicaSets can be deployed directly. But more often than not, they are deployed indirectly via even higher-level objects such as Deployments.</li>
                <li>Deployment: Deployments provide broader desired state management to ReplicaSets. After updating the declarative desired state of replicasets in the JSON or YAML that describes a Deployment, those updates will be rolled out in a controlled fashion (ie a rolling update).</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pods</h2>

        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/podstructure.png" title="Pods" style='max-width:50%'>
        <p><span class='keyword'>Logical host</span> supporting interacting processes</p>

        <aside class='notes'>
            <ul>
                <li>Perhaps the most fundamental conceptual difference between Docker and Kubernetes is the notion of a pod. While Docker's fundamental scheduling unit is a container - a single process with the full stack of isolation technologies applied to it - kubernetes's atomic unit is the pod, which is meant to model a logical host supporting multiple interacting processes.</li>
                <li>At the kernel namespace level, pods reduce some of the isolation between containers by sharing their network, IPC, and UTS namespaces. This allows all the containers within a pod to interact with each other as if they were all sitting on the same host.</li>
                <li>As such, all the containers within a pod share a single IP address, port range, routing table, hostname, and unix sockets. This is the first example of what we meant by kubernetes prioritizing its communication model over its security model.</li>
                <li>Resource limitations by way of control groups are imposed at the pod level; security features like capabilities management, SecComp and security modules are bundled together as securityContext objects (container context takes precedence over pod context).</li>
                <li>All pods contain something called a pause container, which is responsible for keeping the shared namespaces of the pod open even if all other processes in the pod exit or restart.</li>
            </ul>
        </aside>

    </section>    

    <section class="no_bg">
        <h2>Pods</h2>
        <div class="row">
            <div class="col-7">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/pod-addr.png" title="Pods">
            </div>
            <div class="col-5">
                <ul>
                    <li>Use <code>localhost</code> for intra-pod communication</li>
                    <li>All containers in a pod share same IP</li>
                </ul>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Since all containers in a pod share the same network namespace and they all pertain to the same (pod) IP address, containers can just communicate with each other via "localhost".</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pod Networking</h2>
        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/docker-vs-k8s.png" title="Pod Networking">
        <aside class="notes">
            <ul>
                <li>The Docker network:</li>
                <li>On top we have a physical network interface called "eth0". Attached to that is a Linux bridge "docker0", and attached to that is a virtual network interface "veth0". It is important to note that "docker0" and "veth0" are both on the same network, 172.5.0.0/24 in this example. On this network "docker0" is assigned the IP address 172.5.0.1 and it is the default gateway for "veth0", which is assigned the IP address 172.5.0.2.</li>
                <li>The second container gets a new virtual network interface "veth1", connected to the same "docker0" bridge. In our case it is assigned the IP address 172.5.0.3</li>

                <li>Kubernetes Network:</li>
                <li>Docker can start a container and rather than creating a new virtual network interface for it, specify that it shares an existing interface.</li>
                <li>The command looks like this: "docker container run --name bar ... --net container:foo ..."</li>
                <li>The above command shares the network namespace of container "foo" with the new container "bar".</li>
                <li>Now the second container "bar" sees "veth0" rather than getting its own "veth1" as in the previous example.</li>
                <li>In this way multiple containers live in the same network space and can communicate with each other via `localhost`. This is similar to what we know from the situation when we run multiple processes directly on the host.</li>
                <li>Kubernetes implements this pattern by creating a special container "pause" for each pod whose only purpose is to provide a network interface for the other containers.</li>
                <li>The âpauseâ container is the heart of the pod, providing the virtual network interface that all the other containers will use to communicate with each other and the outside world.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Pod Lifecycle</h2>

        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/pod-lifecycle.png" title="Pod Lifecycle">

        <aside class="notes">
            <ul>
                <li>A pod has a lifecycle. Since a it is also an atomic unit of deployment the following is valid:</li>
                <li>Pending: containers are still spinning up (ie being scheduled by kube, and images being downloaded if necessary).</li>
                <li>Running: pod is bound to a node, and at least one container is still running or restarting.</li>
                <li>Succeeded: all containers exited with exit code 0</li>
                <li>Failed: all containers exited, at least one with a non-zero exit code.</li>
                <li>Note the monotonous progression: unlike containers, pods don't stop and restart. They live until they die.</li>
            </ul>
        </aside>
    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/09-kubernetes/images/icon_task.png" class="moby_icon" alt="icon">Instructor Demo: Kubernetes Basics</h2>
        
        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='kubernetes-demo.md'>Kubernetes Basics</li>
        </ul>

        <p>In the Exercises book.</p>
    </section> 

    <section class="no_bg">
        <h2>ReplicaSet</h2>

        <div class="row">
            <div class="col-8">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/replicaset.png" title="ReplicaSet">
            </div>
            <div class="col-4">
                <ul>
                    <li>Scaling</li>
                    <li>Keep-alive</li>
                </ul>
            </div>
        </div>

        <aside class="notes">
            <ul>
                <li>ReplicaSets bring the concepts of desired number of replicas and self-healing to a collection of Pods. Just like in Swarm when a service task dies, a dead pod will be rescheduled by a ReplicaSet.</li>
                <li>We define ReplicaSets with either a YAML or a JSON manifest file and feed it to the API server. This gets handed over to the ReplicaSet controller which makes sure the right number of the right Pod get instantiated. Fundamental to this is the all-powerful reconciliation loop that is constantly watching the cluster and making sure that current state and desired state match.</li>
                <li>Even if we only need a single instance of a Pod, we should probably deploy it via a higher-level object like a ReplicaSet or Deployment. This will give the Pod self-healing capabilities and the ability to scale if we decide we need more in the future.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Deployment</h2>
        <div class="row">
            <div class="col-6">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/deployment.png" title="Deployment">
            </div>
            <div class="col-6">
                <ul>
                    <li>Build on top of <span class="keyword">ReplicaSets</span></li>
                    <li>Add configurable Updates and Rollback</li>
                    <li>Older Versions of ReplicaSets stick around for easy Rollback</li>
                </ul>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Deployments build on top of Pods and ReplicaSets by adding mature and configurable updates and rollbacks.</li>
                <li>Like everything else, theyâre objects in the Kubernetes API and we should be looking to work with them declaratively.</li>
                <li>When we perform updates with the kubectl apply command, older versions of ReplicaSets get wound down, but they stick around making it easy for us to perform rollbacks.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Configuration</h2>

        <p>Applications typically need environment-specific config:</p>
        <ul>
            <li>Environment variables</li>
            <li>Configuration files</li>
            <li>Non-sensitive info (ports, usernames, endpoints)</li>
            <li>Sensitive info (passwords, access tokens, keys)</li>
        </ul>

        <p>Config should be <span class='keyword'>decoupled</span> from pod definition and <span class='keyword'>portable</span> across the cluster.</p>

        <aside class='notes'>
            <ul>
                <li>In addition to scheduling workload as containers and pods, we will typically also need to provide configuration info for our applications.</li>
                <li>We'd like to be able to define the config in a way that keeps it well separated from the definition of our pods, so that the same pods and deployments can be migrated across environments without changing their definition at all; we should be able to swap out modular configuration objects to completely capture changes in config.</li>
                <li>Also, these modular configuration objects need to be managed by our orchestrator so they can be as portable as our pods themselves, able to be provisioned anywhere in our cluster.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>ConfigMaps</h2>

        <ul>
            <li>Collections of key/value pairs, or text files</li>
            <li>Provisioned to containers via env vars or volume mounts</li>
            <li>Appropriate for low/no security config</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>For any non-sensitive configuration information, a configMap satisfies our desires for decoupled, portable configuration.</li>
                <li>ConfigMaps can be defined as lists of key/value pairs, which can be used to populate environment variables in containers, or as lists of files which can be used to populate volume mounts.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Secrets</h2>

        <ul>
            <li>Defined and provisioned similarly to configMaps (env vars or volume mounts)</li>
            <li>Intended for secure info:
                <ul>
                    <li>Provisioned in a tmpfs, never written to disk</li>
                    <li>Encrypted by UCP on masters automatically</li>
                </ul>
            </li>
            <li><span class='keyword'>Warning</span>: secrets are recoverable with <code>kubectl get secrets</code> from masters, and potentially with <code>docker container inspect</code> from host workers</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>For config that requires higher security guarantees, Kubernetes provides a construct parallel to configMaps called secrets.</li>
                <li>Secrets are used similarly to configMaps, in that they are defined separately from pods, and can populate environment variables and mounted files in a running container. Secrets are not, however, written to disk, and are deleted from the tmpfs they are stored in on worker nodes once the container consuming them is deleted.</li>
                <li>Furthermore, UCP sets up default Kube secret encryption on install via the aescbc provider discussed at <a href="https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/"></a>this site.</li>
                <li>Be aware that anyone with `kubectl get secrets` permissions on your cluster can recover the value of a secret from a master. Secrets can also be exposed via environment variables visible through `docker container inspect` on nodes hosting pods that consume a secret as an environment variable. Therefore, while secrets improve the security posture of sensitive information, proper RBAC and limited access to cluster nodes is still crucial for security.</li>
            </ul>
        </aside>
    </section>   

    <section class="no_bg">
        <h2>Storage Volumes</h2>

        <ul>
            <li>Volumes: same lifecycle as pod (compare to persistent Docker volumes)</li>
            <li>PersistentVolumes: 'immortal' volume (similar to Docker)</li>
            <li>Storage backends exposed by <span class='keyword'>container storage interface</span> drivers</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Similar to Docker containers, we often want storage for our applications that outlives the containers that isolate our processes; for this we have Kube volumes.</li>
                <li>One critical difference between Kube and Docker's volume solutions is that Kube volumes are created alongside pods, and do not outlive those pods; once a pod is deleted, the volumes it created are also destroyed. Compare this to a Docker volume, which is created independently from a container and by default outlives any container that mounts it.</li>
                <li>If we want a more 'Docker-like' persistent storage solution, PersistentVolumes provide volumes that are provisioned separately from pods, and persist past the lifecycle of any pods that mount them.</li>
                <li>Kubernetes abstracts away arbitrary third-party storage solutions using the Container Storage Interface, a generic standard for orchestrator storage drivers.</li>
            </ul>
        </aside>

    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/09-kubernetes/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Kubernetes Orchestration</h2>
        
        <p>Work through</p>

        <ul>
            <li class='exercise' script='kube-install.md'>Installing Kubernetes</li>
            <li class='exercise' script='kube-orchestration.md'>Kubernetes Orchestration</li>
            <li class='exercise' script='provisioning-kube-config.md'>Provisioning Kube Config</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>

    </section>

    <section class="no_bg">
        <h2>Kubernetes Network Model</h2>
        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/kube-networking.png" alt="Kube Networking">
        <p>Requirements</p>
        <ul>
            <li>Pod &lt;--&gt; Pod without NAT</li>
            <li>Node &lt;--&gt; Pod without NAT</li>
            <li>Pod's peers find it at the same IP it finds itself</li>
            <li>Creates a <span class='keyword'>flat network</span>, like VMs</li>
        </ul>
        <aside class="notes">
            <ul>
                <li>Kubernetes does not have a strict opinion on how networking between pods has to be implemented. It only requests the following 3 characteristics of a valid networking implementation:</li>
                <li>(1) All pods can communicate with all other pods without NAT</li>
                <li>(2) All nodes running pods can communicate with all pods (and vice-versa) without NAT</li>
                <li>(3) IP that a pod sees itself as is the same IP that other pods see it as</li>
                <li>In our sample we have two nodes on a subnet 172.10.0.0/16 and all pods are on subnet 10.1.0.0/16 while node1 has subnet 10.1.1.0/24 and node2 has 10.1.2.0/24 reserved for its respective pods.</li>
                <li>According to the requirements (1) pod A needs to be able to reach pod B, pod C and pod D without NAT. (2) Furthermore node 1 and 2 need to be able to reach all pods A to D. (3) Finally pod A sees its own IP as 10.1.1.2 and all other pods in the network see pod A with the same IP 10.1.1.2 and can reach it accordingly.</li>
                <li>To elaborate a bit on the 3rd point: processes running inside any container of pod A see their IP as 10.1.1.2.</li>
                <li>Kube's networking model is most distinct from the Docker native CNM in its priorities; Kubernetes wanted a networking model that most closely resembled a flat network of VMs. By demanding all containers sit on a flat network and can all reach each other, Kubernetes forgoes Docker's secure by default firewalling but makes it very simple to port an application previously distributed across networked VMs into a collection of pods that are nearly identical from a networking perspective. This is the other sense in which kubernetes prioritized its communication model over its security model.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Service</h2>
        <div class="row">
            <div class="col-6">
                <p>Problem:</p>
                <ul>
                    <li>Pods are mortal</li>
                    <li>Pods are never resurrected</li>
                    <li>Pod IP Addr cannot be relied on</li>
                </ul>
                <p>Solution:</p>
                <ul>
                    <li><span class"keyword">Service</span> defines: 
                        <ul>
                            <li>Logical set of Pods</li>
                            <li>Policy how to access them</li>
                        </ul> </li>
                </ul>
            </div>
            <div class="col-6">
                <img src="src/modules/Training-for-Containerization/09-kubernetes/images/service.png" title="Service">
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Just like service tasks managed by Swarm, pods managed by Kubernetes come and go, either from failures, scheduling decisions, updates, or scaling.</li>
                <li>We don't want our application logic to have to watch out for these potentially fast-changing operational details; we'd rather abstract away a pool of identical pods behind a stable endpoint, which is what a Kubernetes service provides.</li>
                <li>A kube service is a fully-fledged object in the Kubernetes API just like Pods, ReplicaSets, and Deployments. They provide stable IP addresses, and support TCP and UDP (TCP by default). They also perform simple randomized load-balancing across Pods, though more advanced load balancing algorithms may be supported in the future. This adds up to a situation where Pods can come and go, and the Service automatically updates and continues to provide that stable networking endpoint.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>ClusterIP Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='src/modules/Training-for-Containerization/09-kubernetes/images/clusterip.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster internal origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm VIP</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>The most basic Kubernetes service is the clusterIP service. Traffic sent to the service IP and port will get randomly load balanced across all matching pods, on the service's targetPort port.</li>
                <li>Note this is for cluster internal communications only; the service IP will only be reachable from other pods running on the same cluster.</li>
                <li>Also note, the random routing implies this is appropriate only for stateless destination pods.</li>
                <li>Finally, recall that UCP automatically deploys a kube DNS service - so the IP of your clusterIP services, like all services, can be resolved by a DNS lookup of the service name, again from within the originating pod.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>NodePort Services</h2>

        <div class='row'>
            <div class='col-8'>
                <img src='src/modules/Training-for-Containerization/09-kubernetes/images/nodeport.png'></img>
            </div>
            <div class='col-4'>
                <ul>
                    <li><span class='keyword'>Usecase:</span>
                        <ul>
                            <li>Cluster external origin</li>
                            <li>Stateless destination</li>
                            <li>Similar to Swarm L4 mesh</li>
                        </ul>
                    </li>
                </ul>
            </div>
        </div>

        <aside class='notes'>
            <ul>
                <li>Building off of the clusterIP service is the nodePort service, intended for routing traffic from outside the cluster to your pods.</li>
                <li>nodePort services automatically create clusterIP services that work exactly as described above; the nodePort service itself listens on a randomly assigned port on every node in the cluster, and forwards traffic inbound there to the clusterIP service, which in turn hands it off to the matching pods as above.</li>
                <li>Note that nodePort services listen on EVERY host in the cluster, regardless of what pods are running where; therefore, inbound traffic doesn't need to be targeted at a specific node. Your external load balancer could in principle just fan traffic across the whole cluster on the port selected by your nodePort service, and the Kube services will handle the rest of the routing.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Label Selectors</h2>

        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/labelselector.png" title="label selector" style='width:70%'>

        <aside class='notes'>
            <ul>
                <li>Since a Kubernetes service is a completely independent object, how do we connect it to the pods it's supposed to route traffic to?</li>
                <li>We address this problem with kube label selectors. Pods bear a metadata -> labels key, beneath which can contain any arbitrary label and value; meanwhile, services bear a selector -> matchLabels key beneath which resides the same label and value combination.</li>
                <li>You already saw this in the exercises when you created replicaSets and deployments; the higher level objects were matched to pods in the exact same way. In the examples you declared the pods simultaneously with the orchestration objects, but any pod, even ones declared separately, with a matching label will get picked up and managed by the replicaSet, deployment or service.</li>
                <li>In order to restrict what objects match what other objects in this way, Kubernetes also offers namespacing functionality for objects; in this workshop we'll just put everything in the default namespace, but in practice you might consider running multiple versions of an app (say dev and staging) in different namespaces, so the dev services don't point at staging pods, or vice versa.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Network Policies</h2>
        <div class="row">
            <div class="col-7">
                <ul>
                    <li>Network policies <span class="keyword">control traffic</span></li>
                    <li>Traffic allowed by default</li>
                    <li>Traffic denied if network policy exist but no rule allows it</li>
                    <li>Independent <span class="keyword">ingress</span> &amp; <span class="keyword">egress</span> rules</li>
                </ul>
            </div>
            <div class="col-5">
                <div class="pre large">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: ...
  namespace: ...
  ...
spec:
  <span class="red-bg">podSelector:</span> ...
  <span class="red-bg">ingress:</span>
  - ...
  - ...
  <span class="red-bg">egress:</span>
  - ...
  - ...</div>
            </div>
        </div>
        <aside class="notes">
            <ul>
                <li>Network policies control traffic from and to pods</li>
                <li>Kubernetes defines policies but ignores them silently IF no supporting network plugin is installed.</li>
                <li>Calico is such a network plugin that Docker includes "as batteries included" in UCP 3.0. Calico supports network policies and is commercially backed by Tigera.</li>
                <li>Policies work based on labels (on pods). In a policy we have label selectors that define which pods are affected by the policy</li>
                <li>An empty label selector means that ALL pods are included: matchLabels: {}</li>
                <li>In addition to which pods it applies to a policy also defines which direction of traffic is affected: ingress (inbound traffic; who can access the pod) or egress (outbound traffic; where can the pod connect to)</li>
                <li>On the slide we see the template of a network policy with the podSelector and the ingress and egress rules.</li>
                <li>Traffic is allowed unless there is a network policy selecting the pod</li>
                <li>Traffic is denied if there are policies selecting the pod but none of them have rules allowing it</li>
                <li>We can only write rules to ** allow ** traffic</li>
                <li>Traffic is allowed if there is at least one policy allowing it</li>
                <li>Policies can also restrict port numbers and protocols, e.g. 3456/tcp</li>
                <li>Policy rules are additive (logical OR)</li>
                <li>Multiple pod selectors are also additive (logical OR)</li>
                <li>Network policies are scoped to their namespace (the one they're deployed to)</li>
                <li>Network policies add minimal latency (less than 1 ms overhead): http://blog.kubernetes.io/2016/09/high-performance-network-policies-kubernetes.html</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Sample Network Policies</h2>
        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/network-policies.png" alt="Sample Network Policy">
        <aside class='notes'>
            <ul>
                <li>Here we see a sample network policy that allows some traffic while denying other. Only components from the backend tier can communicate with the DB tier but not e.g. frontend components</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Kubernetes Networking Planes</h2>

        <ul>
            <li>
                Management:
                <ul>
                    <li>Master to Master: etcd Raft</li>
                    <li>Master to Node: apiserver (TCP 443) &lt;-&gt; kubelet (TCP 10250)</li>
                </ul>
            </li>
            <li>
                Data &amp; Control:
                <ul>
                    <li>BYO networking</li>
                    <li>See Cluster DNS, <a href='http://bit.ly/2DMmdyt'>http://bit.ly/2DMmdyt</a></li>
                </ul>
            </li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Like a Swarm, Kubernetes needs a management, control, and data plane to orchestrate fully distributed applications.</li>
                <li>Kube's management plane consists of two distinct parts: etcd replicas across masters maintain a Raft consensus, and masters' apiserver communicates with node's kubelet.</li>
                <li>Unlike Swarm, Kubernetes control and data planes are largely bring-your-own, with the details governed by networking solutions like calico, flannel or weave.</li>
                <li>Also see the Cluster DNS addon for Kubernetes; this will provision a DNS server that registers DNS names for Kubernetes services and pods based on their names and namespaces.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Calico</h2>

        <img src="src/modules/Training-for-Containerization/09-kubernetes/images/calico-routing.png" alt="bgp / ip in ip">

        <aside class='notes'>
            <ul>
                <li>Every kubernetes network plugin implements L3 routing differently; here we illustrate Calico, since that's the one that ships with UCP by default.</li>
                <li>Calico assigns a /26 subnet to each node in the cluster (a node can get additional /26 subnets as they fill up - but a particular /26 will belong to exactly one host.)</li>
                <li>All pods on a host receive IPs in this /26 subnet allocated by the calico IPAM.</li>
                <li>Each pod's network namespace is connected to the host network namespace via a veth, just like in Docker.</li>
                <li>Calico amends the host's routing table to direct traffic bound for a local pod to the corresponding veth endpoint, named 'cali*'.</li>
                <li>Calico also runs a BGP server (BIRD) in an all-to-all mesh, updating the routing tables of all other nodes in the cluster with information about which subnets have been assigned to which nodes. Then, if any traffic is destined for a subnet not corresponding to the local node, the destination node's IP is inferred from the /26 subnet, and the packet is sent via IP-in-IP.</li>
                <li>In this sense, Calico is using BGP as its networking control plane (compare to gossip in Swarm), and IP in IP as its data plane (compare to VXLAN)</li>
                <li>In the case of very large clusters, the all-to-all BGP mesh can become unperformant. It's also possible to configure calico route reflectors to serve as the 'hub' in a hub-and-spoke control plane, where all nodes route outbound traffic to the route reflectors rather than directly to its destination, and the route reflectors then pass the traffic to its destination. This way, the control plane scales linearly in cluster size rather than like n*n, though at the cost of having a central hub for all intra-cluster communications (needs high bandwidth, potential point of failure).</li>
            </ul>
        </aside>

    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/09-kubernetes/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Kubernetes Networking</h2>

        <p>Work through</p> 
        <ul>
            <li class='exercise' script='kube-networking.md'>Kubernetes Networking</li>
        </ul>

        <p>In the Exercises book.</p>
        <h2 class="timer"></h2>

    </section> 
    
    <section class="no_bg">
        <h2>Kubernetes Takeaways</h2>
        <ul>
            <li>Kube provides more flexibility in its orchestration objects at the cost of more config</li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>The key strategic things to understand when thinking about kube are its modularity, and its communication model.</li>
                <li>Compared to Swarm, Kube is much more modular, providing a great deal of flexibility in scheduling decisions. However, as is generally true for most software, added flexibility often results in more complicated config.</li>
                <li>Also comparing to swarm, kube offers a potentially powerful communication model via the concept of pods and its globally reachable network model. If your app requires very chatty containers that need interprocess communication or colocation, pods are an excellent design decision offered by kubernetes.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>
        <ul>
            <li>Docker &amp; Kubernetes: <a href="https://www.docker.com/kubernetes">https://www.docker.com/kubernetes</a></li>
            <li>Official Kubernetes Docs: <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a></li>
            <li>Tutorials: <a href="http://bit.ly/2yLGn61">http://bit.ly/2yLGn61</a></li>
            <li>Interactive Tutorials: <a href="https://bit.ly/2rdwIVZ">https://bit.ly/2rdwIVZ</a></li>
            <li>Understanding Kubernetes Networking: <a href="http://bit.ly/2kdI1qQ">http://bit.ly/2kdI1qQ</a></li>
            <li>Kubernetes the Hard Way: <a href="http://bit.ly/29Dq4wC">http://bit.ly/29Dq4wC</a></li>
        </ul>
        <aside class='notes'>
            <ul>
                <li>Additional resources about Kubernetes</li>
            </ul>
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Fundamental Orchestration Takeaways</h2>

        <ul>
            <li>Distributed Application Architecture orchestrates one or more containers across one or more nodes</li>
            <li>Orchestrators abstract away the differences between processes and between nodes</li>
            <li>Orchestrators enhance scalability and stability</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>We started out this workshop wanting to run any app, anywhere, because process migration is so integral to modern enterprise computing.</li>
                <li>By exploring the concequences of that idea, we came up with this notion of distributed application engineering, that underwrites things like microservices and devops, while enhancing portability, density and security of our apps.</li>
                <li>But when we started thinking in terms of orchestrated containers, we realized the added power and opportunity of shifting our thinking away from individual processes on individual machines, towards understanding our applications as scalable groups of processes (services) and scalable groups of nodes (swarms).</li>
                <li>The portability of containers allows us to schedule tasks in a way that erases the difference between machines in our datacenters, and the immutability and rapid deployment of containers allowed us to create the abstraction of services that are intrinsically more scalable and more robust than any individual process could ever be.</li>
            </ul>
        </aside>

    </section>

</section><section>
    <section class="no_bg">
        <h2>Docker System Commands</h2>
        <aside class="notes">
            <ul>
                <li>So far, we've encountered commands to interact with individual images and containers; in practice however, a Docker workflow will generate a lot of containers and images. In this module, we'll introduce some tools for managing your entire collection of images and containers on a node.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>
        
        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Execute clean-up commands</li>
            <li>Locate Docker system information</li>
        </ul>
    </section>

    <section class="no_bg">   
        <h2>Clean-up Commands</h2>
      
        <ul>
            <li>
                <code>docker system df</code><br>
                <pre>
TYPE           TOTAL    ACTIVE   SIZE        RECLAIMABLE
Images         39       2        9.01 GB     7.269 GB (80%)
Containers     2        2        69.36 MB    0 B (0%)
Local Volumes  0        0        0 B         0 B</pre>
            </li>
            <li><code>docker system prune</code></li>
        </ul>
        <div>
            <p>more limited...</p>
            <ul>
                <li><code>docker image prune [--filter "foo=bar"]</code></li>
                <li><code>docker container prune [--filter "foo=bar"]</code></li>
                <li><code>docker volume prune [--filter "foo=bar"]</code></li>
                <li><code>docker network prune [--filter "foo=bar"]</code></li>
            </ul>
        </div>
        <div class="topcorner"><img src="src/modules/Training-for-Containerization/11-system-commands/images/cleanup.png" alt="Cleanup the System"></div>

        <aside class="notes">
            <ul>
                <li>under heavy use the docker host might consume a load of resources or disk space. To find out with type of elements occupy how much space we can use the <code>docker system df</code> command. It tells us exactly how much space images, container and volumes currently occupy and how much of it is claimable.</li>
                <li>to claim back unused space from the Docker host we can use the command <code>docker system prune</code>. It will try to remove dangling images, stopped containers and unused volumes and networks in one go.</li>
                <li>we also have the more specialized <code>prune</code> commands that only remove unused items of the given type</li>
                <li>Prune commands can also be filtered by label in all cases, before a timestamp via 'until' for everything but volumes, and also by 'dangling' for images, for even more restricted pruning.</li>
            </ul>

            <p>image credit <a href="http://www.wiki.sc4devotion.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/11-system-commands/images/4/41/Wiki_clean.png">http://www.wiki.sc4devotion.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/11-system-commands/images/4/41/Wiki_clean.png</a></p>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Inspect the System</h2>
        <div><code>docker system info</code></div>
        <pre>
Containers: 2
 Running: 2
 Paused: 0
 Stopped: 0
Images: 105
Server Version: 17.03.0-ee
Storage Driver: overlay2
 Backing Filesystem: extfs
 Supports d_type: true
 Native Overlay Diff: true
Logging Driver: json-file
Cgroup Driver: cgroupfs
Plugins:
 Volume: local
 Network: bridge host ipvlan macvlan null overlay
Swarm: active
 NodeID: ybmqksh6fm627armruq0e8id1
 Is Manager: true
 ClusterID: 2rbf1dv6t5ntro2fxbry6ikr3
 Managers: 1
 Nodes: 1
 Orchestration:
  Task History Retention Limit: 5
 Raft:
  Snapshot Interval: 10000
  Number of Old Snapshots to Retain: 0
  Heartbeat Tick: 1
  ...</pre>

        <div class="topcorner"><img src="src/modules/Training-for-Containerization/11-system-commands/images/spyglass.png" alt="Inspect System"></div>

        <aside class="notes">
            <ul>
                <li>We can use the <code>docker system info</code> command to get very detailed information about the current Docker host. This information includes but is not limited to images, containers, swarm mode, networks and volumes.</li>
                <li>
                    When looking at the output, can you identify:
                    <ul>
                        <li>how many images are on your machine?</li>
                        <li>What version of containerd are you running?</li>
                        <li>Whether Docker is running in swarm mode?</li>
                    </ul>
                </li>
                <li>Image from <a href="https://s3-us-west-2.amazonaws.com/nnsrc/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/11-system-commands/images/spiglass.png">https://s3-us-west-2.amazonaws.com/nnsrc/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/11-system-commands/images/spiglass.png</a></li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>System Events</h2>

        <p>
            <span>Start observing with ...</span><br>
            <code>docker system events</code>
        </p>
        <p>Generate events with ...<br>
            <code style="font-size: 0.8em;">docker container run --rm alpine echo 'Hello World!'</code>
        </p>
        <pre>
2017-01-25T16:57:48.553596179-06:00 container create 30eb630790d44052f26c1081...
2017-01-25T16:57:48.556718161-06:00 container attach 30eb630790d44052f26c1081...
2017-01-25T16:57:48.698190608-06:00 network connect de1b2b40f522e69318847ada3...
2017-01-25T16:57:49.062631155-06:00 container start 30eb630790d44052f26c1081d...
2017-01-25T16:57:49.164526268-06:00 container die 30eb630790d44052f26c1081dbf...
2017-01-25T16:57:49.613422740-06:00 network disconnect de1b2b40f522e69318847a...
2017-01-25T16:57:49.815845051-06:00 container destroy 30eb630790d44052f26c108...</pre>

        <div class="topcorner" style="max-width: 35%"><img src="src/modules/Training-for-Containerization/11-system-commands/images/monitoring.png" alt="System Events"></div>
        <aside class="notes">
            <ul>
                <li>`docker system info` gave us some mostly-static metadata about our docker platform; if we want to watch things live, we can take advantage of Docker's event reporting via <code>docker system events</code>.</li>
                <li>please note that the output generated by the <code>docker system events</code> command can also be filtered and custom formatted by using according command arguments <code>--filter</code> and <code>format</code></li>
                <li>Image from <a href="http://www.bing.com/src/modules/Training-for-Containerization/11-system-commands/images/search?view=detailV2&ccid=xHDtNSak&id=F6B851CB5DF9FCB90FD4C4FBF86073617E979328&q=monitoring&simid=608004200364248188&selectedIndex=0&qft=+filterui%3alicense-L1+filterui%3aimagesize-medium&ajaxhist=0">http://www.bing.com/src/modules/fundamentals/system-commands/src/modules/Training-for-Containerization/11-system-commands/images/search?view=detailV2&ccid=xHDtNSak&id=F6B851CB5DF9FCB90FD4C4FBF86073617E979328&q=monitoring&simid=608004200364248188&selectedIndex=0&qft=+filterui%3alicense-L1+filterui%3aimagesize-medium&ajaxhist=0</a></li>
            </ul>            
        </aside>
    </section>

    <section  data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/11-system-commands/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: System Commands</h2>
        <p>Work through</p>
        
        <ul>
            <li class='exercise' script='cleaning-up-docker-resources.md'>Cleaning up Docker Resources</li>
            <li class='exercise' script='inspection-commands.md'>Inspection Commands</li>
        </ul>

        <p>in the Exercises book.<p>
        <h2 class="timer"></h2>

    </section> 

    <section class="no_bg">
        <h2>Discussion</h2>

        <ul>
            <li>What is the origin of dangling image layers?</li>
            <li>What are some potential pitfalls to automating system cleanup with prune commands, and how to avoid them?</li>
            <li>Questions?</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>When a layer is no longer part of any tagged image, typically happens when a name and tag is reused after changing a Dockerfile.</li>
                <li>Deleting containers without grabbing their logs first, deleting volumes that have valuable info just because they weren't currently attached to anything. Avoid by using label-based filters when pruning.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>System commands reference: <a href="http://dockr.ly/2eMR53i">http://dockr.ly/2eMR53i</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about system commands</li>
            </ul>
            
        </aside>
    </section>
</section>
<section>
    <section class="no_bg">
        <h2>Introduction to Docker Compose</h2>
        <aside class="notes">
            <ul>
                <li>Docker provides a number of fundamental tools for approaching the problem of orchestration natively from Docker, and the first of these tools is Compose.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Discussion: Processes vs. Applications</h2>

        <p>Containers and images describe individual processes. What will we need to describe entire applications?</p>

        <aside class='notes'>
            <ul>
                <li>Lead class to the simple answer (some sort of manifest file that describes the application, in the spirit of deploy scripts or infrastructure-as-code), as well as the more substantial answer of some way to manage a living application, where 'manage' in this context means scale, route traffic, deploy and upgrade.</li>
                <li>Hint questions if the class is stuck:</li>
                <li>It's not enough just to 'describe' an application - we need to make the deployment of those applications reproducible and portable. How? (someone should think of some sort of script).</li>
                <li>Are applications static after they're launched? What if load changes? (leads to thinking about scaling).</li>
                <li>After scaling an application, how do we make sure traffic gets to the new instances of our app? Re-do service discovery? Reconfigure load balancers? We'd rather have something a little more transparent.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Learning Objectives</h2>

        <p>By the end of this module, learners will be able to</p>
        <ul>
            <li>Design scalable Docker services</li>
            <li>Leverage Docker's built in service discovery mechanism</li> 
            <li>Write a compose file describing an application</li>
        </ul>

    </section>

    <section class="no_bg">
        <h2>Distributed Application Architecture</h2>

        <ul>
            <li>Applications consisting of one or more containers across one or more nodes</li>
            <li>Docker Compose facilitates multi-container design <span class='keyword'>on a single node</span></li>
        </ul>

        <aside class="notes">
            <ul>
                <li>At this point, we've seen that Docker can provide adequately portable and isolated containers, and we've seen some basic nuts and bolts regarding how those containers can be networked together; we're now ready to start exploring our first orchestration tool for making a true distributed application.</li>
                <li>Ultimately, we'll want to be able to completely decentralize our application, be networking many containers together across many hosts; for now, we'll just solve half the problem, by making an application out of many containers, still all on the same host. We'll relax the single-host constraint in the next chapter.</li>
            </ul>
        </aside>
    </section>

    <section class="no_bg">
        <h2>Docker Services</h2>

            <ul>
                <li>Goal: declare and (re)configure many similar containers all at once</li>
                <li>Goal: scale apps by adding containers seamlessly</li>
                <li>A <span class='keyword'>service</span> defines the <span class='keyword'>desired state</span> of a group of identically configured containers</li>
                <li>Docker provides <span class='keyword'>transparent service discovery</span> for Services</li>
            </ul>

            <aside class='notes'>
                <ul>
                    <li>So far, we've declared containers one at a time with `docker container run...`, and we've seen how to network individual containers together. This all works, but doesn't scale conveniently.</li>
                    <li>Since we're going to start designing apps to consist of potentially many containers, we'd like to be able to create and reconfigure containers en masse.</li>
                    <li>Furthermore, we need to put some thought into how discovery will work in this paradigm; if we scale up an app by declaring more containers, how will they all find out about each other and network themselves together</li>
                    <li>To address this problem, Docker orchestration introduces the idea of services. A service defines the desired state of a collection of identically configured containers, allowing us to declare a batch of containers all at once, and reconfigure them later by updating the service definition.</li>
                    <li>Furthermore, Docker provides out-of-the-box service discovery for services, automatically providing and configuring the networking necessary for these groups of containers to interact.</li>
                </ul>
            </aside>
    </section>  

    <section class="no_bg">
        <h2>Service Discovery</h2>

        <img src='src/modules/Training-for-Containerization/13-compose/images/service-not-process.png' style='width:90%'></img>
        <p>Services are assigned a <span class='keyword'>Virtual IP</span> which spreads traffic out across the underlying containers automatically.</p>

        <aside class='notes'>
            <ul>
                <li>Formerly, we may have had individual processes or containers communicating directly; this isn't practical for a service we want to scale into many processes on demand.</li>
                <li>To address this, Docker assigns a virtual IP to every service, and maintains a DNS lookup table on the host, so that at the application logic level, traffic can be directed to a service as a whole; load balancing to the underlying containers is handled by Docker's onboard VIP server.</li>
                <li>In this way, we can change the number of containers provisioned by a service without needing to do any explicit service discovery in our applications; the application logic sends traffic to the service regardless of how many containers it has provisioned, and Docker does the rest.</li>
            </ul>
        </aside>
    </section>    

    <section class="no_bg">
        <h2>Our Application: Dockercoins</h2>

        <div class='col-6'>
            <img style="background-color:rgba(0,0,0,0); max-width:80%;" src="src/modules/Training-for-Containerization/13-compose/images/dockercoins.png" alt="DockerCoins logo" />
            <p style='font-size: medium !important'>
                (DockerCoins 2016 logo courtesy of <a href="https://twitter.com/xtlcnslt">@XtlCnslt</a> and <a href="https://twitter.com/ndeloof">@ndeloof</a>. Thanks!)
            </p>
        </div>

        <div class='col-6'>
            <ul>
                <li>
                    It is a DockerCoin miner! ð°ð³ð¦ð¢
                </li>
                <li>
                    Dockercoins consists of 5 services working together:
                </li>
            </ul>
            <img src='src/modules/Training-for-Containerization/13-compose/images/dockercoins-flow.png' style='width:70%'></img>
        </div>
    </section>

    <section data-background="#340B65" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/13-compose/images/icon_task.png" class="moby_icon" alt="icon"> Instructor Demo: Docker Compose</h2>

        <p>See the demo</p> 
        
        <ul>
            <li class='demo' script='docker-compose-demo.md'>Docker Compose</li>
        </ul>

        <p>In the Exercises book.</p>
    </section>

    <section data-background="#00a2a1" class="green_bg">
        <h2><img src="src/modules/Training-for-Containerization/13-compose/images/icon_task.png" class="moby_icon" alt="icon"> Exercise: Compose Apps</h2>
        <p>Work through</p>
        <ul>
            <li class='exercise' script='starting-a-compose-app.md'>Starting a Compose App</li>
            <li class='exercise' script='scaling-a-compose-app.md'>Scaling a Compose App</li>
        </ul>

        <p>in the Exercises book.</p>
        <h2 class="timer"></h2>
    </section> 

    <section class="no_bg">
        <h2>Docker Compose Takeaways</h2>

        <ul>
            <li>Docker Compose makes single node orchestration easy</li>
            <li>Compose services makes scaling applications easy</li>
            <li>Bottleneck identification important</li>
            <li>Syntactically: <code>docker-compose.yml</code> + API</li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>Compose should be your go-to native Docker solution for orchestrating services and containers on a single node.</li>
                <li>In the next section, we'll learn how to do the same across many nodes.</li>
            </ul>
        </aside>

    </section>

    <section class="no_bg">
        <h2>Further Reading</h2>

        <ul>
            <li>Docker compose examples: <a href="http://dockr.ly/1FL2VQ6">http://dockr.ly/1FL2VQ6</a></li>
            <li>Overview of docker-compose CLI: <a href="http://dockr.ly/2wtQlZT">http://dockr.ly/2wtQlZT</a></li>
            <li><code>docker-compose.yaml</code> reference: <a href="http://dockr.ly/2iHUpeX">http://dockr.ly/2iHUpeX</a></li>
            <li>Docker Compose and Windows: <a href="http://bit.ly/2watrqk">http://bit.ly/2watrqk</a></li>
        </ul>

        <aside class='notes'>
            <ul>
                <li>additional resources about Docker compose</li>
            </ul>
            
        </aside>
    </section>
</section><section>
    <section class="no_bg">
        <h2>Wrap Up 2 - Sonarqube</h2>
    </section>
    <section data-background="#00a2a1" class="green_bg">
        <h2>Exercise Instructions</h2>
        <ul>
            <li>Setup a Sonarqube server that listens on port 9000</li>
            <li>Connect it to a persistent database<br>i.e. if you âdocker rm âfâ your Sonarqube container and run a new one, no data is lost</li>
            <li>Use postgresql and persist itâs data on the host filesystem using volumes</li>
            <li>Verify e.g. by creating a user via Sonarqube UI, remove the container and run a new one â is the user still present?</li>
            <li>Check that Sonarqube is really using your postgresql database
            </li>
            <li><b>Hint</b>: use docker-compose</li>
        </ul>
        <br>
        <br>
        <h2 class="timer"></h2>
        <aside class="notes">
            Check that Sonarqube is really using your postgresql database: 
                <ul>
                    <li>login to postgres container</li>
                    <li>psql sonar sonar</li>
                    <li>\l - list all existing databases</li>
                    <li>\dt - list all tables of databases</li>
                    <li><b>Hint</b>: Postgresql tips at http://www.unixwitch.de/de/sysadmin/tools/postgres</li>
                </ul>
        </aside>
    </section>
    
    <section class="no_bg">
        <h2>Solution</h2>
        <div class='pre' style="font-size:small;">version: "2"
services:
  sonarqube:
    image: sonarqube
    ports:
      - "9000:9000"
    networks:
      - sonarnet
    environment:
      - SONARQUBE_JDBC_URL=jdbc:postgresql://db:5432/sonar
    volumes:
      - sonarqube_conf:/opt/sonarqube/conf
      - sonarqube_data:/opt/sonarqube/data
      - sonarqube_extensions:/opt/sonarqube/extensions
      - sonarqube_bundled-plugins:/opt/sonarqube/lib/bundled-plugins

  db:
    image: postgres
    networks:
      - sonarnet
    environment:
      - POSTGRES_USER=sonar
      - POSTGRES_PASSWORD=sonar
    volumes:
      - postgresql:/var/lib/postgresql
      - postgresql_data:/var/lib/postgresql/datanetworks:
    sonarnet:
      driver: bridge

volumes:
  sonarqube_conf:
  sonarqube_data:
  sonarqube_extensions:
  sonarqube_bundled-plugins:
  postgresql:
  postgresql_data:</div>

    </section>
</section><section class="no_bg">
    <h2>Containerization Training</h2>
    <p>Please take our feedback survey</p>
    <p><a href='https://bit.ly/3fBp1gh'>https://bit.ly/3fBp1gh</a></p>
    <p>Get in touch: office@gepardec.com</p>
    <p><a href='https://www.gepardec.com/trainings'>https://www.gepardec.com/trainings</a></p>
</section>



<script>
    // Set the date we're counting down to
    
    function addMinutes(date, minutes) {
      return new Date(date.getTime() + minutes*60000);
    }

    var countDownDate = addMinutes( new Date(), 22);
    
    // Update the count down every 1 second
    var x = setInterval(function() {
    
      // Get today's date and time
      var now = new Date().getTime();
        
      // Find the distance between now and the count down date
      var distance = countDownDate - now;
        
      // Time calculations for days, hours, minutes and seconds
      var days = Math.floor(distance / (1000 * 60 * 60 * 24));
      var hours = Math.floor((distance % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
      var minutes = Math.floor((distance % (1000 * 60 * 60)) / (1000 * 60));
      var seconds = Math.floor((distance % (1000 * 60)) / 1000);
    

      var el = document.getElementsByClassName("timer");
      for (var i = 0, ilen = el.length; i < ilen; i++) {
        el[i].innerHTML = minutes + "m " + seconds + "s ";
        // If the count down is over, write some text 
        if (distance < 0) {
          clearInterval(x);
          el[i].innerHTML = "Time is up!";
        }
      }
      
    }, 1000);
</script>
            </div>
        </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.js"></script>

        <script>

            // Full list of configuration options available at:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: true,

                transition: 'slide', // none/fade/slide/convex/concave/zoom

                menu: {
                    themes: false,
                    transitions: false,
                },

                // Optional reveal.js plugins
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true },
                    { src: 'plugin/notes/notes.js', async: true }
                ]/*,

                // uncomment this block to make left/right advance slide and up/down advance chapter.
                keyboard: {
                    39: 'next',
                    37: 'prev',
                    38: 'left',
                    40: 'right'
                }
                */
            });

        </script>

    </body>
</html>